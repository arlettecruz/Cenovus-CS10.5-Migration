{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab50e98",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5a35e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.5.2\n",
      "cs-Oracle version: 8.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as dom\n",
    "import cx_Oracle as ora\n",
    "import pyodbc as sql\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'cs-Oracle version: {ora.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94969578",
   "metadata": {},
   "source": [
    "### 1.1 Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a131bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:42:57 PM: Done\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters\n",
    "\n",
    "#env = 'QA'\n",
    "#env = 'STG'\n",
    "env = 'PRD'\n",
    "\n",
    "path = 'C:\\\\Users\\\\aolguin\\\\Projects\\\\CVE\\\\'\n",
    "extract_path = 'Extracts\\\\'\n",
    "xml_path = 'Build\\\\OI Scripts\\\\'\n",
    "upload_path = '\\\\\\wsidm009pd\\\\CS10\\\\'\n",
    "mapping_file = 'Build\\\\Mappings\\\\Mapping Tables.xlsx'\n",
    "\n",
    "source_path = {'FinancialImages_AD': ':Financial Images_Migrated to CDMS & redirecting:'\n",
    "               ,'CorporateFinance': ':Corporate Finance_NOMIGRATE:'\n",
    "               ,'JDEFinancialPDFs': ':JDE Financial PDFs and Reports_TO BE ARCHIVED:'\n",
    "               ,'FinancialImages_UO': ':Financial Images:'\n",
    "               ,'IncomeAccounting': ':Royalty Income Accounting:'\n",
    "               ,'JDEAttachments': ':JDE Attachments:'\n",
    "              }\n",
    "target_path = {'FinancialImages_AD': 'Enterprise:Department Administration:Finance:z-Archives:(Application Data) Financial Images:'\n",
    "               ,'CorporateFinance': 'Enterprise:Department Administration:Finance:z-Archives:(Corporate Finance) Comptrollers Corporate Finance:'\n",
    "               ,'JDEFinancialPDFs': 'Enterprise:Department Administration:Finance:z-Archives:(Corporate Finance) JDE Financial PDFs:'\n",
    "               ,'FinancialImages_UO': 'Enterprise:Department Administration:Finance:z-Archives:(Upstream Operations) Central Services Financial Images:'\n",
    "               ,'IncomeAccounting': 'Enterprise:Department Administration:Finance:z-Archives:(Upstream Operations) Central Services Royalty Income Accounting:'\n",
    "               ,'JDEAttachments': 'Enterprise:Department Administration:Finance:z-Archives:(Upstream Operations) Supply Mgmt JDE Attachments:'\n",
    "              }\n",
    "source_parentid = {'FinancialImages_AD': 67272049\n",
    "                   ,'CorporateFinance': 1221831\n",
    "                   ,'JDEFinancialPDFs': 257997\n",
    "                   ,'FinancialImages_UO': 1234487\n",
    "                   ,'IncomeAccounting': 177197\n",
    "                   ,'JDEAttachments': 171378752\n",
    "                  }\n",
    "\n",
    "df_extract = {}\n",
    "df_cat_extract = {}\n",
    "df_report = {}\n",
    "df_xml = {}\n",
    "\n",
    "target_folder = 'Enterprise:Department Administration:Finance:z-Archives:'\n",
    "batch_size = 20000\n",
    "\n",
    "cnxn_str = {'cdms': 'Driver={SQL Server Native Client 11.0}; Server=csagprd01list; Database=ContentServer; UID=csReadOnly; PWD=Ry62sW781010@@',\n",
    "#            'cve': 'cs10/cs10tqa55@ORAPRD24:1521/ORAPRD24?encoding=UTF-8&nencoding=UTF-8'\n",
    "            'cve': 'cs10:cs10tqa55@ORAPRD24:1521/ORAPRD24?encoding=UTF-8&nencoding=UTF-8'\n",
    "           }\n",
    "query = {}\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe3a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:42:57 PM: Done\n"
     ]
    }
   ],
   "source": [
    "# Create Database Engines\n",
    "#ora_engine = create_engine('oracle://' + cnxn_str['cve'], use_nchar_for_unicode=True, coerce_to_unicode=False, text_encoding_errors='replace')\n",
    "ora_engine = create_engine('oracle://' + cnxn_str['cve'], encoding_errors='replace')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c515b9",
   "metadata": {},
   "source": [
    "### 1.2 Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a7f303",
   "metadata": {},
   "source": [
    "#### extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea473aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:42:59 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def extract_data (collection:str, folder_name: str, dataid: int, save_to_file: bool):\n",
    "    \n",
    "    # Build query strings\n",
    "    query['containers'] = f\"\"\"\n",
    "            SELECT  dt.DataId\n",
    "                    ,dt.VersionNum\n",
    "                    ,dt.ParentId\n",
    "                    ,dt.Name\n",
    "                    ,nn.NickName\n",
    "                    ,dt.dcomment\n",
    "                    ,dt.createdate\n",
    "                    ,dt.modifydate\n",
    "                    ,cb.name        createdby\n",
    "                    ,CASE SubType   WHEN 136 THEN 'compounddoc'    WHEN 557 THEN 'compoundemail'\n",
    "                                    WHEN 0 THEN 'folder'           WHEN 202 THEN 'project' \n",
    "                                    WHEN 751 THEN 'emailfolder'    END SubType\n",
    "            --                ,REGEXP_REPLACE(Name, '[^[:print:]]', '') Name\n",
    "                    ,LEVEL FolderLevel\n",
    "                    ,SYS_CONNECT_BY_PATH(REPLACE(dt.Name, ':', ' '), ':') path\n",
    "            FROM    DTree dt\n",
    "            LEFT OUTER JOIN NickName nn ON nn.Id = dt.DataId\n",
    "            LEFT OUTER JOIN Kuaf cb ON dt.createdby = cb.Id\n",
    "            WHERE   SubType IN (0, 751, 202, 136, 557)\n",
    "            START WITH DataID = {dataid}\n",
    "            CONNECT BY NOCYCLE PRIOR DataID = ParentID\n",
    "            \"\"\"\n",
    "    query['contents'] = f\"\"\"\n",
    "            SELECT  dt.DataId\n",
    "                    ,dt.VersionNum\n",
    "                    ,dt.ParentId\n",
    "                    ,CASE dt.SubType WHEN 1 THEN 'alias'              WHEN 140 THEN 'url'\n",
    "                                     WHEN 144 THEN 'document'            WHEN 749 THEN 'email'      END SubType\n",
    "                    ,REPLACE(REGEXP_REPLACE(dt.Name, '[^[:print:]]', ''), '?', '') Name\n",
    "                    ,nn.NickName\n",
    "                    ,dt.dcomment\n",
    "                    ,dt.createdate\n",
    "                    ,dt.modifydate\n",
    "                    ,cb.name        createdby\n",
    "                    ,dt.exatt1      url\n",
    "                    ,dt.origindataid\n",
    "                    ,LEVEL FolderLevel\n",
    "                    ,SYS_CONNECT_BY_PATH(REPLACE(dt.Name, ':', ' '), ':') path\n",
    "                    ,vd.VersionId\n",
    "                    ,vd.Version\n",
    "                    ,REPLACE(REPLACE(REPLACE(REGEXP_REPLACE(filename, '[^[:print:]]', ''), '?', ''), 'â€™', ''), '%', '') FileName\n",
    "                    ,vd.MimeType\n",
    "                    ,vd.DataSize\n",
    "                    ,vd.VercDate\n",
    "                    ,vd.VermDate\n",
    "                    ,pd.ProviderData\n",
    "            FROM    DTree dt\n",
    "            LEFT OUTER JOIN NickName nn ON nn.Id = dt.DataId\n",
    "            LEFT OUTER JOIN Kuaf cb ON dt.createdby = cb.Id\n",
    "            LEFT OUTER JOIN (SELECT * FROM DVersData WHERE VerType IS NULL) vd ON vd.DocId = dt.DataId AND vd.Version = dt.VersionNum\n",
    "            LEFT OUTER JOIN ProviderData pd ON pd.ProviderId = vd.ProviderId\n",
    "            WHERE   dt.SubType IN (1, 144, 140, 749)\n",
    "            START WITH dt.DataID = {dataid}\n",
    "            CONNECT BY NOCYCLE PRIOR dt.DataID = dt.ParentID\n",
    "            \"\"\"\n",
    "        \n",
    "    # Upload Data from Database\n",
    "    cnxn = ora_engine.connect()\n",
    "    df_extract[collection] = pd.read_sql(query[collection], cnxn)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded {folder_name} = {df_extract[collection].shape[0]} rows')\n",
    "    if save_to_file:\n",
    "        df_extract[collection].to_csv(path + extract_path + 'Extracted_' + folder_name + '_' + collection + '.csv', index=False)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Saving to csv {folder_name}= {df_extract[collection].shape[0]} rows')\n",
    "#        release_extract_memory([collection])\n",
    "    cnxn.close()\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3387121",
   "metadata": {},
   "source": [
    "#### extract_category_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbcfbf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:00 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def extract_category_data (category: dict):\n",
    "    catid = category['catid']\n",
    "    catname = category['catname']\n",
    "    df_cat_extract[catname] = pd.DataFrame()\n",
    "    for attribute in [i for i in category if i not in ['catid', 'catname']]:\n",
    "        attid = category[attribute][0]\n",
    "        colname = category[attribute][1]\n",
    "        query['cat_att'] = f\"\"\"\n",
    "                SELECT  Id\n",
    "                        ,VerNum\n",
    "                        ,{colname} \"{attribute}\"\n",
    "                FROM    LLAttrData \n",
    "                WHERE   defid = {catid}\n",
    "                AND     AttrId = {attid}\n",
    "                AND     {colname} IS NOT NULL\n",
    "                \"\"\"        \n",
    "        # Upload Data from Database\n",
    "        cnxn = ora_engine.connect()\n",
    "        df = pd.read_sql(query['cat_att'], cnxn)\n",
    "        if df_cat_extract[catname].empty:\n",
    "            df_cat_extract[catname] = df\n",
    "        else:\n",
    "            df_cat_extract[catname] = df_cat_extract[catname].merge(df, how='outer', on=['id', 'vernum'], copy=False) \n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded {attribute} = {df.shape[0]} rows')\n",
    "        cnxn.close()\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4cc31",
   "metadata": {},
   "source": [
    "#### apply_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af45a212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:02 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def apply_transformation(collection:str):\n",
    "    \n",
    "    # batch\n",
    "    df_batches = pd.DataFrame(df_extract[collection].groupby(by='subtype').cumcount().apply(lambda x: int(x/batch_size+1)), columns=['batch'])\n",
    "    if 'batch' in df_extract[collection].columns: df_extract[collection].drop(['batch'], axis=1, inplace=True)\n",
    "    df_extract[collection] = df_extract[collection].merge(df_batches, how='left', left_index=True, right_index=True)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     batch number = {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "    # c_location\n",
    "    #df_extract[collection]['c_location'] = df_extract[collection][['path', 'name']].apply(lambda x: str(x['path']).replace(source_path[folder], target_path[folder]).replace(':' + x['name'], ''), axis=1)\n",
    "    df_extract[collection]['c_location'] = df_extract[collection]['path'].apply(lambda x: x.replace(source_path[folder], target_path[folder])[::-1][x[::-1].find(':')+1:][::-1])\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_location... {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "    # c_metadata\n",
    "    migration_date = dt.now().strftime(\"%Y-%b-%d %H:%M:%S\")\n",
    "    df_extract[collection]['c_metadata'] = df_extract[collection][['modifydate', 'createdby', 'nickname']].apply(lambda x: f'<MigrationDate={migration_date}><ModifiedDate={x[\"modifydate\"]}><CreatedBy={x[\"createdby\"]}><NickName={x[\"nickname\"]}>' if pd.notnull(x[\"nickname\"])\n",
    "                                                                                                                 else f'<MigrationDate={migration_date}><ModifiedDate={x[\"modifydate\"]}><CreatedBy={x[\"createdby\"]}>', axis=1)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_metadata... {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "    if collection == 'containers':\n",
    "        # c_createdate\n",
    "        df_extract[collection]['c_createdate'] = df_extract[collection][['createdate']].apply(lambda x: str(x['createdate']).replace('-', '').replace(' ', '').replace(':', ''), axis=1)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_createdate... {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "        # c_modifydate\n",
    "        df_extract[collection]['c_modifydate'] = df_extract[collection][['modifydate']].apply(lambda x: str(x['modifydate']).replace('-', '').replace(' ', '').replace(':', ''), axis=1)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_modifydate... {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "    if collection == 'contents':\n",
    "        # c_url\n",
    "        df_extract[collection]['c_url'] = df_extract[collection][['dataid']][(df_extract[collection]['subtype'] == 'alias')].apply(lambda x: 'https://contentserver.cenovus.com/otcs2/cs.exe/open/' + str(x['dataid']), axis=1)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_url... {df_extract[collection].shape[0]} rows affected')\n",
    "        \n",
    "        # c_createdate\n",
    "        df_extract[collection]['c_createdate'] = df_extract[collection][['createdate', 'vercdate', 'version']].apply(lambda x: str(x['vercdate']).replace('-', '').replace(' ', '').replace(':', '') \n",
    "                                                                                                                     if x['version'] > 1 \n",
    "                                                                                                                     else str(x['createdate']).replace('-', '').replace(' ', '').replace(':', ''), axis=1)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_createdate... {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "        # c_modifydate\n",
    "        df_extract[collection]['c_modifydate'] = df_extract[collection][['modifydate', 'vermdate', 'version']].apply(lambda x: str(x['vermdate']).replace('-', '').replace(' ', '').replace(':', '')\n",
    "                                                                                                                     if x['version'] > 1 \n",
    "                                                                                                                     else str(x['modifydate']).replace('-', '').replace(' ', '').replace(':', ''), axis=1)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_modifydate... {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "        # c_sourcefile\n",
    "        df_extract[collection]['c_source_filepath'] = df_extract[collection][['providerdata']][(df_extract[collection]['providerdata'].notnull())].apply(lambda x: re.findall(\"(?<=providerInfo'=')[0-9\\\\\\\\.dat]*\", x['providerdata'])[0] if x['providerdata'].find('providerInfo') != -1 else x['providerdata'], axis=1)\n",
    "        df_extract[collection]['c_providername'] = df_extract[collection][['providerdata']][(df_extract[collection]['providerdata'].notnull())].apply(lambda x: re.findall(\"(?<=subProviderName'=')[a-zA-Z0-9]*\", x['providerdata'])[0] if x['providerdata'].find('subProviderName') != -1 else 'Default', axis=1)\n",
    "        df_extract[collection]['c_sourcefile'] = df_extract[collection][['c_source_filepath', 'c_providername']].apply(lambda x: '\\\\\\\\n01svmnas1\\\\contentserver_prd02\\\\' + x['c_source_filepath'] if x['c_providername'] == 'Default'\n",
    "                                                                                                                       else ('\\\\\\\\n01svmnas1\\\\contentserver_prd01\\\\' + x['c_source_filepath'] if x['c_providername'] == 'Vol01'\n",
    "                                                                                                                             else ''), axis=1)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_sourcefile = {df_extract[collection].shape[0]} rows affected')\n",
    "        \n",
    "        # c_targetfile\n",
    "        #df_extract[collection]['c_target_filepath'] = df_extract[collection][['c_source_filepath', 'dataid', 'filename']][(df_extract[collection]['providerdata'].notnull())].apply(lambda x: x['c_source_filepath'].replace(str(int(x['dataid']))+'.dat', x['filename']), axis=1)\n",
    "        #df_extract[collection]['c_targetfile'] = df_extract[collection][['batch', 'c_target_filepath']][(df_extract[collection]['providerdata'].notnull())].apply(lambda x: f\"{upload_path}{folder}\\\\{str(x['batch'])}\\\\{x['c_target_filepath']}\", axis=1)    \n",
    "        df_extract[collection]['c_targetfile'] = df_extract[collection][['dataid', 'batch', 'version', 'filename']][(df_extract[collection]['providerdata'].notnull())].apply(lambda x: f\"{upload_path}{folder}\\\\{str(x['batch'])}\\\\({str(int(x['dataid']))}-{str(int(x['version']))}) - {x['filename']}\", axis=1)    \n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     c_targetfile = {df_extract[collection].shape[0]} rows affected')\n",
    "\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753ee9",
   "metadata": {},
   "source": [
    "#### map_to_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcad779e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:04 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def map_to_xml(collecion:str, subtype:str, category:dict):\n",
    "    df = df_extract[collection][(df_extract[collection]['subtype'] == subtype)]\n",
    "    xml = {}\n",
    "    nodes = len(df)\n",
    "    xml = {'title': df['name'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'location': df['c_location'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'description': df['dcomment'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'created': df['c_createdate'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'createdby': ['Admin'] * nodes,\n",
    "           'modified': df['c_modifydate'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           # Id's\n",
    "           'batch': df['batch'],\n",
    "           'dataid': df['dataid'],\n",
    "           'level': df['folderlevel'],\n",
    "           # Migration System Properties\n",
    "           'Source System': ['CS 10.5'] * nodes,\n",
    "           'System ID': df['dataid'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Metadata': df['c_metadata'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "          }\n",
    "    if subtype == 'url':\n",
    "        xml['url'] = df['url'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "    if subtype == 'alias':\n",
    "        xml['url'] = df['c_url'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "    if subtype in ['document', 'email']:\n",
    "        # Migration System Properties\n",
    "        xml['Version'] = df['version'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "        # Id's\n",
    "        xml['version'] = df['version']\n",
    "        # Version\n",
    "        xml['filename'] = df['filename'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "        xml['file'] = df['c_targetfile'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "        xml['mime'] = df['mimetype'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "    if any(category):\n",
    "        for attribute in [i for i in category if i not in ['catid', 'catname']]:\n",
    "            if category[attribute][1] == 'ValInt':\n",
    "                xml[attribute] = df[attribute].apply(lambda x: str(int(x)).strip() if pd.notnull(x) else '')\n",
    "            else:\n",
    "                xml[attribute] = df[attribute].apply(lambda x: str(x).strip() if pd.notnull(x) else '')\n",
    "\n",
    "    df_xml[subtype] = pd.DataFrame(xml)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {subtype} = {df_xml[subtype].shape[0]} rows')\n",
    "   \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba452c",
   "metadata": {},
   "source": [
    "#### build_create_oi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec9c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:05 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def build_create_oi(subtype: str, output_file: str, sort_list: list, legacy_id: str, categories: dict):\n",
    "    df = df_xml[subtype]\n",
    "    for batch in df['batch'][(df['batch'].notnull())].sort_values().unique():\n",
    "        LegacyID_ant = ''\n",
    "        xml_file_name = 'create-' + subtype + '-' + output_file + '-' + str(batch).zfill(2) + '.xml'\n",
    "        df_batch = df[(df['batch'] == batch)].sort_values(by=sort_list)\n",
    "        root = ET.Element('import')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for key, value in df_batch.iterrows():\n",
    "            node = ET.Element('node')\n",
    "            node.set('type', subtype)\n",
    "            location = ET.SubElement(node, 'location')\n",
    "            location.text = value['location']\n",
    "            title = ET.SubElement(node, 'title')\n",
    "            title.text = value['title']\n",
    "            ET.SubElement(node, 'description').text = value['description']\n",
    "            ET.SubElement(node, 'created').text = value['created']\n",
    "            ET.SubElement(node, 'createdby').text = value['createdby']\n",
    "            ET.SubElement(node, 'modified').text = value['modified']\n",
    "            if subtype in ['document', 'email', 'caddocument']:\n",
    "                ET.SubElement(node, 'file').text = value['file']\n",
    "                ET.SubElement(node, 'filename').text = value['filename']\n",
    "                ET.SubElement(node, 'mime').text = value['mime']                    \n",
    "                if value[legacy_id] == LegacyID_ant:\n",
    "                    node.set('action', 'addversion')\n",
    "                    node.remove(location)\n",
    "                    node.remove(title)\n",
    "                    ET.SubElement(node, 'location').text = value['location'] + ':' + value['title']\n",
    "                else:\n",
    "                    node.set('action', 'create')\n",
    "                LegacyID_ant = value[legacy_id]\n",
    "            if subtype == 'url':\n",
    "                node.set('action', 'create')\n",
    "                ET.SubElement(node, 'url').text = value['url']\n",
    "            if subtype in ('folder', 'emailfolder'):\n",
    "                node.set('action', 'create')\n",
    "            if subtype == 'alias':\n",
    "                node.set('type', 'url')\n",
    "                node.set('action', 'create')\n",
    "                ET.SubElement(node, 'url').text = value['url']\n",
    "    #            ET.SubElement(node, 'alias').text = value['alias']\n",
    "            if subtype in ['compounddoc', 'compoundemail']:\n",
    "                node.set('action', 'create')\n",
    "            if len(categories) != 0:\n",
    "                for category_name in categories:\n",
    "                    category = ET.Element('category')\n",
    "                    category.set('name', category_name)\n",
    "                    cat_atts = categories[category_name]\n",
    "                    att_count = 0\n",
    "                    for att in cat_atts:\n",
    "                        if len(value[att]) > 0: \n",
    "                            ET.SubElement(category, 'attribute', attrib={'name': att}).text = value[att]\n",
    "                            att_count += 1\n",
    "                    if att_count >0: node.append(category)\n",
    "            root.append(node)\n",
    "        xml = ET.tostring(root, encoding='utf-8', method='xml')\n",
    "        xml_parsed = dom.parseString(xml).toprettyxml()\n",
    "        xml_file = open(path + xml_path + xml_file_name, 'w',  encoding='utf-8')\n",
    "        xml_file.write(xml_parsed)\n",
    "        xml_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {xml_file_name} = {len(root)} nodes out of {df_batch.shape[0]}')\n",
    "        root.clear()\n",
    "        \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343d042",
   "metadata": {},
   "source": [
    "#### create_batch_files() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76148c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:07 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def create_batch_files (folder: str, df: pd.core.frame.DataFrame):\n",
    "    batches = df['batch'][(df['batch'].notnull())].sort_values().unique()\n",
    "    for batch in batches:\n",
    "        df_batch = df[df['batch'] == batch]\n",
    "        bat_file_name = 'copy-' + folder + '-' + str(batch).zfill(2)\n",
    "        bat_file = open(path + xml_path + bat_file_name  + '.bat', 'w',  encoding='utf-8')\n",
    "        cmd = f'ECHO start %time% >> \"{bat_file_name}.log\"'\n",
    "        bat_file.write(cmd + '\\n')    \n",
    "        for key, val in df_batch.iterrows():\n",
    "            cmd = f'ECHO \"{val[\"dataid\"]} - {val[\"filename\"]}\" >> \"{bat_file_name}.log\"'\n",
    "            bat_file.write(cmd + '\\n')\n",
    "            cmd = f'IF EXIST \"{val[\"c_sourcefile\"]}\" (ECHO F | XCOPY /Y /Q /F \"{val[\"c_sourcefile\"]}\" \"{val[\"c_targetfile\"]}\" >> \"{bat_file_name}.log\") ELSE (ECHO \"{val[\"c_sourcefile\"]}\" does not exist) >> \"{bat_file_name}.log\"'\n",
    "            bat_file.write(cmd + '\\n')\n",
    "        cmd = f'ECHO end %time% >> \"{bat_file_name}.log\"'\n",
    "        bat_file.write(cmd + '\\n')    \n",
    "        bat_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {bat_file_name} = {df_batch.shape[0]}')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238be3e8",
   "metadata": {},
   "source": [
    "#### release_extract_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d775ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:09 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def release_extract_memory(names:list):\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Releasing Memory...')\n",
    "    \n",
    "    total_memory_used = 0\n",
    "    memory_freed = 0\n",
    "    for name in names:\n",
    "        print(f'Extract - {name}')\n",
    "        # Calculate used memory\n",
    "        memory_used = df_extract[name].memory_usage().sum()\n",
    "        memory_used = memory_used/1024/1024\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Usage = {memory_used} MB')\n",
    "        # Delete DataFrames\n",
    "        del df_extract[name]\n",
    "        df_extract[name] = pd.DataFrame()\n",
    "        memory_freed = gc.collect()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Freed = {memory_freed}')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03b1db",
   "metadata": {},
   "source": [
    "#### release_cat_extract_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab8835cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:11 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def release_cat_extract_memory(names:list):\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Releasing Memory...')\n",
    "    \n",
    "    total_memory_used = 0\n",
    "    memory_freed = 0\n",
    "    for name in names:\n",
    "        print(f'Extract - {name}')\n",
    "        # Calculate used memory\n",
    "        memory_used = df_cat_extract[name].memory_usage().sum()\n",
    "        memory_used = memory_used/1024/1024\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Usage = {memory_used} MB')\n",
    "        # Delete DataFrames\n",
    "        del df_cat_extract[name]\n",
    "        df_cat_extract[name] = pd.DataFrame()\n",
    "        memory_freed = gc.collect()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Freed = {memory_freed}')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b837a",
   "metadata": {},
   "source": [
    "#### release_xml_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef1db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:13 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def release_xml_memory():\n",
    "    for collection in df_xml:\n",
    "        print(f'XML - {collection}')\n",
    "        # Calculate used memory\n",
    "        memory_used = df_xml[collection].memory_usage().sum()\n",
    "        memory_used = memory_used/1024/1024\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Usage = {memory_used} MB')\n",
    "        # Delete DataFrames\n",
    "        del df_xml[collection]\n",
    "        memory_freed = gc.collect()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Freed = {memory_freed}')\n",
    "        df_xml[collection] = pd.DataFrame()\n",
    "#    df_xml = {}\n",
    "    \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab60b7",
   "metadata": {},
   "source": [
    "# 2. Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857935a",
   "metadata": {},
   "source": [
    "### 2.1 Set Folder parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c2d41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 12:43:17 PM: Folder: FinancialImages_UO\n",
      "Oct 31 2023 12:43:17 PM: Category: Accounts Payable\n",
      "Oct 31 2023 12:43:17 PM: Done\n"
     ]
    }
   ],
   "source": [
    "folder = ''\n",
    "category = {}\n",
    "##folder = 'FinancialImages_AD'\n",
    "##folder = 'CorporateFinance'\n",
    "##folder = 'JDEFinancialPDFs'\n",
    "folder = 'FinancialImages_UO'\n",
    "##folder = 'IncomeAccounting'\n",
    "##folder = 'JDEAttachments'\n",
    "\n",
    "if folder == 'JDEFinancialPDFs':\n",
    "    # JDE Financial Reports\n",
    "    category = {'catid': 257759,\n",
    "                'catname': 'JDE Financial Reports',\n",
    "                'Report Type': (2, 'ValStr'),\n",
    "                'User Id': (3, 'ValStr'),\n",
    "                'Version': (4, 'ValStr'),\n",
    "                'Queue': (5, 'ValStr'),\n",
    "                'Date': (6, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\"),\n",
    "                'Host': (7, 'ValStr')\n",
    "               }\n",
    "if folder in ['FinancialImages_AD', 'CorporateFinance', 'FinancialImages_UO']:\n",
    "    # Accounts Payable\n",
    "    category = {'catid': 152558,\n",
    "                'catname': 'Accounts Payable',\n",
    "                'EnCana Company Number': (2, 'ValStr'),\n",
    "                'EnCana Company Name': (3, 'ValStr'),\n",
    "                'Invoice Number': (5, 'ValStr'),\n",
    "                'Invoice Date': (6, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\"),\n",
    "                'Supplier Number': (7, 'ValStr'),\n",
    "                'Supplier Name': (8, 'ValStr'),\n",
    "                'JDE Document ID': (9, 'ValStr'),\n",
    "                'JDE Document Type': (10, 'ValStr'),\n",
    "                'JDE Batch Number': (11, 'ValStr'),\n",
    "                'Gross Amount': (12, 'ValStr'),\n",
    "                'Invoice Type': (13, 'ValStr'),\n",
    "                'Pay Item': (14, 'ValStr'),\n",
    "                'G/L Class Code': (15, 'ValStr'),\n",
    "                'Currency': (16, 'ValStr'),\n",
    "                'Suspense Account': (17, 'ValStr'),\n",
    "                'G/L Date': (18, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\"),\n",
    "                'Department Code': (19, 'ValStr'),\n",
    "                'Department Description': (20, 'ValStr'),\n",
    "                'Scan Date': (21, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\"),\n",
    "                'Supporting': (22, 'ValInt'),\n",
    "                'Box ID': (23, 'ValStr'),\n",
    "                'Created By': (24, 'ValStr'),\n",
    "                'Manual Payments': (25, 'ValInt'),\n",
    "                'Service Date': (26, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\")\n",
    "               }\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Folder: {folder}')\n",
    "if any(category): print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Category: {category[\"catname\"]}')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96ea81",
   "metadata": {},
   "source": [
    "### 2.2 Relase Memory if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad838d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#release_extract_memory(['containers'])\n",
    "#release_extract_memory(['containers', 'contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c4154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#release_cat_extract_memory(['Accounts Payable1', 'Accounts Payable2', 'Accounts Payable3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43727e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release_xml_memory()\n",
    "#del df_xml\n",
    "#df_xml = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea05bc",
   "metadata": {},
   "source": [
    "### 2.2 Container and Content data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd431e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 31 2023 14:58:31 PM: Loading contents for FinancialImages_UO...\n"
     ]
    }
   ],
   "source": [
    "# Container and Content data\n",
    "#for collection in ['containers', 'contents']:\n",
    "for collection in ['contents']:\n",
    "#for collection in ['containers']:\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading {collection} for {folder}...')\n",
    "    extract_data (collection, folder, source_parentid[folder], True)\n",
    "    df_extract[collection].sort_values(by=['subtype', 'path'], inplace=True)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container data from flat files\n",
    "print(f'Loading Containers...')\n",
    "df_extract['containers'] = pd.read_csv(path + extract_path + 'Extracted_' + folder_name + '_containers.csv', dtype={'dataid': 'Int64', 'versionnum': 'Int64', 'parentid': 'Int64', 'name': 'string', 'nickname': 'string', 'dcomment': 'string', 'createdate': 'string', 'modifydate': 'string', 'createdby': 'string', 'subtype': 'string', 'folderlevel': 'Int64', 'path': 'string'})\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded {folder_name} = {df_extract['containers'].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf005cd",
   "metadata": {},
   "source": [
    "### 2.3 Category data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47943b",
   "metadata": {},
   "source": [
    "#### 2.3.a Load Accounts Payable category from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef12da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Category data from flat files\n",
    "print(f'Loading Accounts Payable1...')\n",
    "df_cat_extract['Accounts Payable1'] = pd.read_csv(path + extract_path + 'Extracted_category_Accounts Payable1.csv', dtype={'id': 'Int64', 'vernum': 'Int64', 'EnCana Company Number': 'string', 'EnCana Company Name': 'string', 'Invoice Number': 'string', 'Invoice Date': 'string', 'Supplier Number': 'string', 'Supplier Name': 'string', 'JDE Document ID': 'string', 'JDE Document Type': 'string'})\n",
    "#df_extract['Accounts Payable1']= pd.read_csv(path + extract_path + 'Extracted_category_Accounts Payable1.csv', dtype='string')\n",
    "print(f'Loading Accounts Payable2...')\n",
    "df_cat_extract['Accounts Payable2'] = pd.read_csv(path + extract_path + 'Extracted_category_Accounts Payable2.csv', dtype={'id': 'Int64', 'vernum': 'Int64', 'JDE Batch Number': 'string', 'Gross Amount': 'string', 'Invoice Type': 'string', 'Pay Item': 'string', 'G/L Class Code': 'string', 'Currency': 'string', 'Suspense Account': 'string', 'G/L Date': 'string'})\n",
    "#df_extract['Accounts Payable2']= pd.read_csv(path + extract_path + 'Extracted_category_Accounts Payable2.csv', dtype='string')\n",
    "print(f'Loading Accounts Payable3...')\n",
    "df_cat_extract['Accounts Payable3'] = pd.read_csv(path + extract_path + 'Extracted_category_Accounts Payable3.csv', dtype={'id': 'Int64', 'vernum': 'Int64', 'Department Code': 'string', 'Department Description': 'string', 'Scan Date': 'string', 'Supporting': 'Int64', 'Box ID': 'string', 'Created By': 'string', 'Manual Payments': 'Int64', 'Service Date': 'string'})\n",
    "#df_extract['Accounts Payable3']= pd.read_csv(path + extract_path + 'Extracted_category_Accounts Payable3.csv', dtype='string')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e90c6",
   "metadata": {},
   "source": [
    "#### 2.3.b Load Accounts Payable category in 3 pieces or JDE Financial Reports from database and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d488e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {}\n",
    "category = {'catid': 152558,\n",
    "            'catname': 'Accounts Payable1',\n",
    "            'EnCana Company Number': (2, 'ValStr'),\n",
    "            'EnCana Company Name': (3, 'ValStr'),\n",
    "            'Invoice Number': (5, 'ValStr'),\n",
    "            'Invoice Date': (6, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\"),\n",
    "            'Supplier Number': (7, 'ValStr'),\n",
    "            'Supplier Name': (8, 'ValStr'),\n",
    "            'JDE Document ID': (9, 'ValStr'),\n",
    "            'JDE Document Type': (10, 'ValStr')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfe75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {'catid': 152558,\n",
    "            'catname': 'Accounts Payable2',\n",
    "            'JDE Batch Number': (11, 'ValStr'),\n",
    "            'Gross Amount': (12, 'ValStr'),\n",
    "            'Invoice Type': (13, 'ValStr'),\n",
    "            'Pay Item': (14, 'ValStr'),\n",
    "            'G/L Class Code': (15, 'ValStr'),\n",
    "            'Currency': (16, 'ValStr'),\n",
    "            'Suspense Account': (17, 'ValStr'),\n",
    "            'G/L Date': (18, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\")\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {'catid': 152558,\n",
    "            'catname': 'Accounts Payable3',\n",
    "            'Department Code': (19, 'ValStr'),\n",
    "            'Department Description': (20, 'ValStr'),\n",
    "            'Scan Date': (21, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\"),\n",
    "            'Supporting': (22, 'ValInt'),\n",
    "            'Box ID': (23, 'ValStr'),\n",
    "            'Created By': (24, 'ValStr'),\n",
    "            'Manual Payments': (25, 'ValInt'),\n",
    "            'Service Date': (26, \"TO_CHAR(ValDate, 'YYYYMMDDHHMISS')\")\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category data from Database\n",
    "if any(category):\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading category {category[\"catname\"]}...')\n",
    "    extract_category_data(category)\n",
    "\n",
    "    #Save to file if needed\n",
    "    df_cat_extract[category[\"catname\"]].to_csv(path + extract_path + f'Extracted_category_' + category[\"catname\"] + '.csv', index=False)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988be3fe",
   "metadata": {},
   "source": [
    "# 3. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to metadata - For JDE Attachments\n",
    "if any(category):\n",
    "    for collection in ['containers', 'contents']:\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Joining {collection} with {category[\"catname\"]} for {folder}...')\n",
    "        df_extract[collection] = df_extract[collection].merge(df_cat_extract[category[\"catname\"]], how='left', left_on=['dataid', 'versionnum'], right_on=['id', 'vernum'])\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {collection}-{category[\"catname\"]}... {df_extract[collection].shape[0]} rows affected')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23363e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to metadata - For Accounts Payable split in 3 dataframes\n",
    "#for collection in ['containers', 'contents']:\n",
    "#for collection in ['contents']:\n",
    "for collection in ['containers']:\n",
    "    for catname in ['Accounts Payable1', 'Accounts Payable2', 'Accounts Payable3']:\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Joining {collection} with {catname} for {folder}...')\n",
    "        df_extract[collection] = df_extract[collection].merge(df_cat_extract[catname], how='left', left_on=['dataid', 'versionnum'], right_on=['id', 'vernum'])\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {collection}-{catname}... {df_extract[collection].shape[0]} rows affected')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b75e40",
   "metadata": {},
   "source": [
    "### 3.1 Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870e615",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply Transformation\n",
    "#for collection in ['containers', 'contents']:\n",
    "#for collection in ['contents']:\n",
    "for collection in ['containers']:\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Transforming {collection} for {folder}...')\n",
    "    apply_transformation(collection)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c825d",
   "metadata": {},
   "source": [
    "### 3.2 Validation & Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e721f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for objtype in ['containers', 'contents']:\n",
    "for objtype in ['containers']:\n",
    "    # 3.3.1.a Duplicated DataIds in EXTRACT\n",
    "    # (cartesian product)\n",
    "    report_name = f'dup_dataid-{objtype}'\n",
    "    df_report[report_name] = df_extract[objtype][['dataid', 'name']].groupby(by='dataid').count()\n",
    "    df_report[report_name] = df_report[report_name][(df_report[report_name]['name'] > 1)]\n",
    "    print(f'Duplicated DataIds: \\n {df_report[report_name]}')\n",
    "\n",
    "    # 3.3.2 Duplicated Document Names in same Folder\n",
    "    report_name = f'dup_docs-{objtype}'\n",
    "    df_report[report_name] = df_extract[objtype][['c_location', 'name', 'subtype']].groupby(by=['c_location', 'name']).count()\n",
    "    df_report[report_name] = df_report[report_name][(df_report[report_name]['subtype'] > 1)]\n",
    "    print(f'\\n Duplicated File Names within Target Folders: \\n {df_report[report_name]}')\n",
    "    \n",
    "    # Duplicate details\n",
    "    #df_extract['containers'].merge(df_report['dup_docs-containers'], how='right', on=['c_location', 'name']).to_excel(path+'dups.xlsx')\n",
    "\n",
    "    # 3.3.6 Counts per SUBTYPE\n",
    "    report_name = f'count_per_subtype-{objtype}'\n",
    "    if objtype == 'contents': df_report[report_name] = df_extract[objtype][['subtype', 'dataid', 'versionid', 'nickname', 'datasize']].groupby(by=['subtype'], dropna=False).agg({'dataid': pd.Series.nunique, 'versionid': pd.Series.nunique, 'nickname': pd.Series.count, 'datasize': lambda x: np.sum(x)/1024/1024/1024})\n",
    "    if objtype == 'containers': df_report[report_name] = df_extract[objtype][['subtype', 'dataid', 'nickname']].groupby(by=['subtype'], dropna=False).agg({'dataid': pd.Series.nunique, 'nickname': pd.Series.count})\n",
    "    df_report[report_name].rename(columns={'subtype': 'Type', 'dataid': '# Items', 'nickname': 'With Nickname'}, inplace=True)\n",
    "    print(f'\\n Counts per SubType: \\n {df_report[report_name]}')\n",
    "\n",
    "    # 3.3.6 Counts per SUBTYPE and BATCH\n",
    "    report_name = f'count_per_batch-{objtype}'\n",
    "    if objtype == 'contents': df_report[report_name] = df_extract[objtype][['subtype', 'batch', 'dataid', 'versionid', 'nickname', 'datasize']].groupby(by=['subtype', 'batch'], dropna=False).agg({'dataid': pd.Series.nunique, 'versionid': pd.Series.nunique, 'nickname': pd.Series.count, 'datasize': lambda x: np.sum(x)/1024/1024/1024})\n",
    "    if objtype == 'containers': df_report[report_name] = df_extract[objtype][['subtype', 'batch', 'dataid', 'nickname']].groupby(by=['subtype', 'batch'], dropna=False).agg({'dataid': pd.Series.nunique, 'nickname': pd.Series.count})\n",
    "    df_report[report_name].rename(columns={'subtype': 'Type', 'dataid': '# Items', 'nickname': 'With Nickname'}, inplace=True)\n",
    "\n",
    "with pd.ExcelWriter(path + f'CS 10.5 - Counts - {folder}.xlsx') as writer:\n",
    "    for report in df_report:\n",
    "        df_report[report].to_excel(writer, sheet_name=report)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26888fb",
   "metadata": {},
   "source": [
    "# 4. Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca2c68",
   "metadata": {},
   "source": [
    "### 4.1 Batch Files - XCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3746a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Creating copy batch files {folder}...')\n",
    "\n",
    "df = df_extract['contents'][(df_extract['contents']['subtype'].isin(['document', 'email']))]\n",
    "create_batch_files (folder, df)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621dab7d",
   "metadata": {},
   "source": [
    "### 4.2 Object Importer Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XML DataFrames\n",
    "\n",
    "#for collection in ['containers', 'contents']:\n",
    "for collection in ['containers']:\n",
    "#for collection in ['contents']:\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Creating {collection} xmls dataframes for {folder}...')\n",
    "    for subtype in df_extract[collection]['subtype'].unique():\n",
    "        map_to_xml(collection, subtype, category)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d82768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate OI Files\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Creating OI files for {folder}...')\n",
    "for subtype in df_xml.keys():\n",
    "    print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {subtype} = {df_xml[subtype].shape[0]} rows')\n",
    "    sortby = ['location', 'title']\n",
    "    if 'version' in df_xml[subtype]: \n",
    "        sortby = ['location', 'dataid', 'version']\n",
    "    build_create_oi(subtype = subtype,\n",
    "                    output_file = folder,\n",
    "                    sort_list = sortby,\n",
    "                    legacy_id = 'dataid'\n",
    "                    ,categories = {'Content Server Categories:Migration System Properties': ['Source System', 'System ID', 'Metadata']\n",
    "                                 ,'Content Server Categories:CS10 Migration:Accounts Payable': ['EnCana Company Number', 'EnCana Company Name', 'Invoice Number', 'Invoice Date', 'Supplier Number', 'Supplier Name',\n",
    "                                                                                                'JDE Document ID', 'JDE Document Type', 'JDE Batch Number', 'Gross Amount', 'Invoice Type',\n",
    "                                                                                                'Pay Item', 'G/L Class Code', 'Currency', 'Suspense Account', 'G/L Date', 'Department Code',\n",
    "                                                                                                'Department Description', 'Scan Date', 'Supporting', 'Box ID', 'Created By', 'Manual Payments','Service Date']\n",
    "#                                 ,'Content Server Categories:CS10 Migration:JDE Financial Reports': ['Report Type', 'User Id', 'Version', 'Queue', 'Date', 'Host']\n",
    "                                  }\n",
    "                   )\n",
    "\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4135e6",
   "metadata": {},
   "source": [
    "# ----- SAMPLE CODE -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_update_cats_oi(df: pd.DataFrame, type: str, output_file: str, sort_list: list, category_name: str, cat_atts: list):\n",
    "    for batch in df['batch'][(df['batch'].notnull())].sort_values().unique():\n",
    "        LegacyID_ant = ''\n",
    "        xml_file_name = 'update-' + type + '-' + output_file + '-' + str(batch).zfill(2) + '.xml'\n",
    "        df_batch = df[(df['batch'] == batch)].sort_values(by=sort_list)\n",
    "        root = ET.Element('import')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for key, value in df_batch.iterrows():\n",
    "            node = ET.Element('node')\n",
    "            node.set('type', type)\n",
    "            node.set('action', 'update')\n",
    "            ET.SubElement(node, 'location').text = value['location'] + ':' + value['title']\n",
    "#            ET.SubElement(node, 'location').text = value['location']\n",
    "#            ET.SubElement(node, 'description').text = value['description']\n",
    "            if len(cat_atts) != 0 and category_name != '':\n",
    "                category = ET.Element('category')\n",
    "                category.set('name', category_name)\n",
    "                for att in cat_atts:\n",
    "                    if len(value[att]) > 0:\n",
    "#                        print(f'{value[att]} {len(value[att])}')\n",
    "                        ET.SubElement(category, 'attribute', attrib={'name': att}).text = value[att]\n",
    "                node.append(category)\n",
    "            root.append(node)\n",
    "        xml = ET.tostring(root, encoding='utf-8', method='xml')\n",
    "        xml_parsed = dom.parseString(xml).toprettyxml()\n",
    "        xml_file = open(path + xml_path + xml_file_name, 'w',  encoding='utf-8')\n",
    "        xml_file.write(xml_parsed)\n",
    "        xml_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {xml_file_name} = {len(root)} nodes out of {df_batch.shape[0]}')\n",
    "        root.clear()\n",
    "        \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbcf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.3 Update DOCUMENTS to add Legacy eB Web category\n",
    "df = pd.DataFrame()\n",
    "for ft in ['Y', 'N - SPEC', 'N - REFERENCE', 'N - BORE', 'N - PROCEDURE', 'UNKNOWN']:\n",
    "    df = df_xml[ft]\n",
    "    if not df.empty:\n",
    "        print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {ft} = {df.shape[0]} rows')\n",
    "        build_update_oi(df = df,\n",
    "                        type = 'document',\n",
    "                        output_file = ft,\n",
    "                        sort_list = ['location', 'title'],\n",
    "                        category_name = 'Content Server Categories:Legacy eB Web',\n",
    "                        cat_atts = ['Document Id', 'File Id', 'Repository', 'Path', 'File Name', 'File Size']\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_delete_oi(df: pd.DataFrame, type: str, output_file: str, sort_list: list):\n",
    "    for batch in df['batch'][(df['batch'].notnull())].sort_values().unique():\n",
    "        xml_file_name = 'delete-' + type + '-' + output_file + '-' + str(batch).zfill(2) + '.xml'\n",
    "        df_batch = df[(df['batch'] == batch)].sort_values(by=sort_list, ascending=False)\n",
    "        root = ET.Element('import')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for key, value in df_batch.iterrows():\n",
    "            node = ET.Element('node')\n",
    "            node.set('type', type)\n",
    "            node.set('action', 'delete')\n",
    "            ET.SubElement(node, 'location').text = value['location'] + ':' + value['title']\n",
    "#            ET.SubElement(node, 'title').text = value['title']\n",
    "            root.append(node)\n",
    "        xml = ET.tostring(root, encoding='utf-8', method='xml')\n",
    "        xml_parsed = dom.parseString(xml).toprettyxml()\n",
    "        xml_file = open(path + xml_path + xml_file_name, 'w',  encoding='utf-8')\n",
    "        xml_file.write(xml_parsed)\n",
    "        xml_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {xml_file_name} = {len(root)} nodes out of {df_batch.shape[0]}')\n",
    "        root.clear()\n",
    "        \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4c4d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.4.3 Delete DOCUMENTS\n",
    "for ft in  ['Y', 'N - SPEC', 'N - REFERENCE', 'N - BORE', 'N - PROCEDURE', 'UNKNOWN']:\n",
    "    df = df_xml[ft][['location', 'title', 'batch']].drop_duplicates()\n",
    "    if not df.empty:\n",
    "        print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {ft} = {df.shape[0]} rows')\n",
    "        build_delete_oi(df = df,\n",
    "                        type = 'document',\n",
    "                        output_file = ft,\n",
    "                        sort_list = ['location', 'title']\n",
    "                       )\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab50e98",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5a35e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.5.2\n",
      "cs-Oracle version: 8.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as dom\n",
    "import cx_Oracle as ora\n",
    "import pyodbc as sql\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'cs-Oracle version: {ora.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94969578",
   "metadata": {},
   "source": [
    "### 1.1 Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a131bb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 13 2023 13:44:16 PM: Done\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters\n",
    "\n",
    "#env = 'QA'\n",
    "#env = 'STG'\n",
    "env = 'PRD'\n",
    "\n",
    "path = 'C:\\\\Users\\\\aolguin\\\\Projects\\\\CVE\\\\'\n",
    "extract_path = path + 'Extracts\\\\'\n",
    "xml_path = 'Build\\\\OI Scripts\\\\'\n",
    "mapping_file = 'Build\\\\Mappings\\\\Mapping Tables.xlsx'\n",
    "JDE_files = {'jdepddta': 'Extracts\\\\jdepddta_f00165_links-2.csv'\n",
    "            ,'jdelgdta': 'Extracts\\\\jdelgdta_f00165_links-2.csv'\n",
    "            }\n",
    "MAXIMO_files = {'maximo1': 'Extracts\\\\docinfo_1.csv',\n",
    "                'maximo2': 'Extracts\\\\docinfo_2.csv',\n",
    "                'maximo3': 'Extracts\\\\docinfo_3.csv'                \n",
    "               }\n",
    "#extract_path = 'E:\\\\eBWeb Test Export\\\\Projects\\\\'\n",
    "#upload_path = '\\\\\\wsidm009pd\\\\eBHuskyDevice\\\\Projects\\\\'\n",
    "\n",
    "df_extract = {}\n",
    "df_report = {}\n",
    "df_JDE_report = {}\n",
    "\n",
    "target_folder = 'Enterprise:Department Administration:Finance:z-Archives:'\n",
    "#batch_size = 6000\n",
    "\n",
    "cnxn_str = {'cdms': 'Driver={SQL Server Native Client 11.0}; Server=csagprd01list; Database=ContentServer; UID=csReadOnly; PWD=Ry62sW781010@@',\n",
    "#            'cve': 'cs10/cs10tqa55@ORAPRD24:1521/ORAPRD24?encoding=UTF-8&nencoding=UTF-8'\n",
    "            'cve': 'cs10:cs10tqa55@ORAPRD24:1521/ORAPRD24?encoding=UTF-8&nencoding=UTF-8'\n",
    "           }\n",
    "\n",
    "query = {}\n",
    "query['cdms'] = \"\"\"\n",
    "        SELECT  ValStr DATAID_CVE, MAX(Id) DATAID_CDMS\n",
    "        FROM    LLAttrData\n",
    "        WHERE   DefId = 253905\n",
    "        AND     AttrId = 2\n",
    "        AND     ValStr IS NOT NULL\n",
    "        AND     ISNUMERIC(ValStr) = 1\n",
    "        GROUP BY ValStr\n",
    "        \"\"\"\n",
    "query['zmaster'] = \"\"\"\n",
    "        SELECT  OldDataID\n",
    "        FROM    Z_MIGRATION_MASTER\n",
    "        \"\"\"\n",
    "query['cve_root_folders'] = \"\"\"\n",
    "        SELECT  DataId, Name\n",
    "        FROM    dtree\n",
    "        WHERE   parentid = 2000\n",
    "        AND     subtype = 0\n",
    "        ORDER BY Name\n",
    "        \"\"\"\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe3a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 13 2023 13:44:17 PM: Done\n"
     ]
    }
   ],
   "source": [
    "# Create Database Engines\n",
    "#ora_engine = create_engine('oracle://' + cnxn_str['cve'], use_nchar_for_unicode=True, coerce_to_unicode=False, text_encoding_errors='replace')\n",
    "ora_engine = create_engine('oracle://' + cnxn_str['cve'], encoding_errors='replace')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c515b9",
   "metadata": {},
   "source": [
    "### 1.2 Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea473aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 25 2023 14:25:31 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def upload_cve_data (dataid: int, save_to_file: bool):\n",
    "    \n",
    "    # Build query strings\n",
    "    query['cve_containers'] = f\"\"\"\n",
    "        WITH    folders AS\n",
    "                (SELECT  CONNECT_BY_ROOT Name RootFolder\n",
    "                        ,ParentId\n",
    "                        ,DataId\n",
    "                        ,REGEXP_REPLACE(Name, '[^[:print:]]', '') Name\n",
    "                        ,CASE SubType WHEN 0 THEN 'Folder' WHEN 202 THEN 'Project' WHEN 751 THEN 'Email Folders'   END SubType\n",
    "                        ,LEVEL FolderLevel\n",
    "                        ,SYS_CONNECT_BY_PATH(REPLACE(Name, ':', ' '), ':') path\n",
    "                 FROM   DTree\n",
    "                 WHERE  SubType IN (0, 202, 751)\n",
    "                 START WITH DataID = {dataid}\n",
    "                 CONNECT BY NOCYCLE PRIOR DataID = ParentID)\n",
    "        SELECT  RootFolder\n",
    "                ,CASE   WHEN FolderLevel = 2 AND INSTR(path, ':', 1, 3) = 0 THEN SUBSTR(path, INSTR(path, ':', 1, 2)+1)\n",
    "                        ELSE SUBSTR(path, INSTR(path, ':', 1, 2)+1, INSTR(path, ':', 1, 3)-INSTR(path, ':', 1, 2)-1) END    Level2\n",
    "                ,CASE   WHEN FolderLevel = 3 AND INSTR(path, ':', 1, 4) = 0 THEN SUBSTR(path, INSTR(path, ':', 1, 3)+1)\n",
    "                        ELSE SUBSTR(path, INSTR(path, ':', 1, 3)+1, INSTR(path, ':', 1, 4)-INSTR(path, ':', 1, 3)-1) END    Level3\n",
    "                ,CASE   WHEN FolderLevel = 4 AND INSTR(path, ':', 1, 5) = 0 THEN SUBSTR(path, INSTR(path, ':', 1, 4)+1)\n",
    "                        ELSE SUBSTR(path, INSTR(path, ':', 1, 4)+1, INSTR(path, ':', 1, 5)-INSTR(path, ':', 1, 4)-1) END    Level4\n",
    "--                ,CASE   WHEN FolderLevel = 5 AND INSTR(path, ':', 1, 6) = 0 THEN SUBSTR(path, INSTR(path, ':', 1, 5)+1)\n",
    "--                        ELSE SUBSTR(path, INSTR(path, ':', 1, 5)+1, INSTR(path, ':', 1, 6)-INSTR(path, ':', 1, 5)-1) END    Level5\n",
    "--                ,FolderLevel\n",
    "--                ,ParentId\n",
    "                ,DataId\n",
    "--                ,Name\n",
    "--                ,SubType\n",
    "                ,Path\n",
    "        FROM    Folders\n",
    "        \"\"\"\n",
    "    query['cve_contents'] = f\"\"\"\n",
    "        SELECT  dt.ParentId\n",
    "                ,dt.DataId\n",
    "                ,vd.VersionId\n",
    "                ,vd.Version\n",
    "                ,CASE dt.SubType WHEN 1 THEN 'Shortcut'              WHEN 140 THEN 'Url'\n",
    "                                 WHEN 136 THEN 'Compound Document'   WHEN 144 THEN 'Document'\n",
    "                                 WHEN 557 THEN 'Compound Email'      WHEN 749 THEN 'Email'\n",
    "                                 WHEN 411 THEN 'Physical Item'\n",
    "                                 WHEN 0 THEN 'Folder'                WHEN 202 THEN 'Project' \n",
    "                                 WHEN 751 THEN 'Email Folders'                                  END SubType\n",
    "                ,vd.DataSize\n",
    "        FROM    DTree dt\n",
    "        LEFT OUTER JOIN DVersData vd ON vd.DocId = dt.DataId\n",
    "        WHERE   dt.SubType IN (1, 136, 140, 144, 411, 557, 749, 0, 202, 751)\n",
    "        AND     dt.Deleted = 0\n",
    "        START WITH dt.DataID = {dataid}\n",
    "        CONNECT BY NOCYCLE PRIOR dt.DataID = dt.ParentID\n",
    "        \"\"\" \n",
    "    \n",
    "    # Upload Data from Database\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading CVE Folder and Document data...')\n",
    "#    cnxn = ora.connect(cnxn_str['cve'])\n",
    "    cnxn = ora_engine.connect()\n",
    "    for i in ['cve_containers', 'cve_contents']:\n",
    "        df_extract[i] = pd.read_sql(query[i], cnxn)\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded {i}= {df_extract[i].shape[0]} rows')\n",
    "        if save_to_file:\n",
    "            df_extract[i].to_csv(extract_path + i + '_' + str(dataid) + '.csv')\n",
    "            print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Saving to csv {i}= {df_extract[i].shape[0]} rows')\n",
    "            release_extract_memory([i])\n",
    "    cnxn.close()\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842756d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 13 2023 14:20:05 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def upload_cve_cats (dataid: int):\n",
    "    \n",
    "    # Build query strings\n",
    "    query['cve_categories'] = f\"\"\"\n",
    "        WITH documents AS\n",
    "                (SELECT  DataId\n",
    "                        ,VersionNum\n",
    "                 FROM \tDTree dt\n",
    "                 WHERE \tSubType IN (1, 136, 140, 144, 411, 557, 749)\n",
    "                 START WITH DataID = {dataid}\n",
    "                 CONNECT BY NOCYCLE PRIOR DataID = ParentID)\n",
    "        SELECT  ValStr\n",
    "                ,COUNT(DISTINCT DataId) items\n",
    "        FROM    llAttrData att\n",
    "        JOIN    documents do ON do.DataId = att.Id AND do.VersionNum = att.Vernum\n",
    "        WHERE   AttrId = 1\n",
    "        GROUP BY ValStr\n",
    "        ORDER BY ValStr\n",
    "        \"\"\"\n",
    "    \n",
    "    # Upload Data from Database\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading CVE Folder and Document data...')\n",
    "#    cnxn = ora.connect(cnxn_str['cve'])\n",
    "    cnxn = ora_engine.connect()\n",
    "    df_extract['cve_categories'] = pd.read_sql(query['cve_categories'], cnxn)\n",
    "    cnxn.close()\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded categories = {df_extract[\"cve_categories\"].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d3c3ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 25 2023 15:04:32 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def create_data_assessment_report(root_folder:str):\n",
    "\n",
    "    # 1. Merge with CDMS, JDE, MAXIMO TABLES\n",
    "\n",
    "    # Merge CDMS\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Merging CDMS...')\n",
    "    if 'DATAID_CVE' in df_extract['cve_contents'].columns: df_extract['cve_contents'].drop(['DATAID_CVE'], axis=1, inplace=True)\n",
    "    if 'DATAID_CDMS' in df_extract['cve_contents'].columns: df_extract['cve_contents'].drop(['DATAID_CDMS'], axis=1, inplace=True)\n",
    "    df_extract['cve_contents'] = df_extract['cve_contents'].merge(df_extract['CDMS'], how='left', left_on='dataid', right_on= 'DATAID_CVE')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Merged CDMS = {df_extract[\"cve_contents\"].shape[0]} rows')\n",
    "\n",
    "    # Merge ZMASTER\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Merging ZMASTER...')\n",
    "    if 'DATAID_ZMASTER' in df_extract['cve_contents'].columns: df_extract['cve_contents'].drop(['DATAID_ZMASTER'], axis=1, inplace=True)\n",
    "    df_extract['cve_contents'] = df_extract['cve_contents'].merge(df_extract['ZMASTER'], how='left', left_on='dataid', right_on= 'DATAID_ZMASTER')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Merged ZMASTER = {df_extract[\"cve_contents\"].shape[0]} rows')\n",
    "\n",
    "    # Merge JDE\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Merging JDE...')\n",
    "    if 'DATAID_JDE' in df_extract['cve_contents'].columns: df_extract['cve_contents'].drop(['DATAID_JDE'], axis=1, inplace=True)\n",
    "    df_extract['cve_contents'] = df_extract['cve_contents'].merge(df_extract['JDE'], how='left', left_on='dataid', right_on= 'DATAID_JDE')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Merged JDE = {df_extract[\"cve_contents\"].shape[0]} rows')\n",
    "\n",
    "    # Merge MAXIMO\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Merging MAXIMO...')\n",
    "    if 'DATAID_MAX' in df_extract['cve_contents'].columns: df_extract['cve_contents'].drop(['DATAID_MAX'], axis=1, inplace=True)\n",
    "    df_extract['cve_contents'] = df_extract['cve_contents'].merge(df_extract['MAXIMO'], how='left', left_on='dataid', right_on= 'DATAID_MAX')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Merged MAXIMO = {df_extract[\"cve_contents\"].shape[0]} rows')\n",
    "\n",
    "\n",
    "    # 2. Generate Summary Matrix per PARENTID\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Grouping per ParentID...')\n",
    "\n",
    "    # Group CONTENTS by PARENTID\n",
    "    df_extract['cve_contents'].sort_values(by=['parentid', 'subtype'], inplace=True, na_position='first')\n",
    "    df_content_sum = df_extract['cve_contents'][['parentid', 'subtype', 'dataid', 'versionid', 'datasize', 'DATAID_ZMASTER', 'DATAID_CDMS', 'DATAID_JDE', 'DATAID_MAX']].groupby(by=['parentid', 'subtype'], dropna=False).agg({'dataid': pd.Series.nunique, 'versionid': pd.Series.count, 'datasize': lambda x: np.sum(x)/1024/1024/1024, 'DATAID_ZMASTER': pd.Series.nunique, 'DATAID_CDMS': pd.Series.nunique, 'DATAID_JDE': pd.Series.nunique, 'DATAID_MAX': pd.Series.nunique})\n",
    "    df_content_sum.reset_index(inplace=True)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Grouped cve_contents = {df_content_sum.shape[0]} rows')\n",
    "\n",
    "    # Transverse the grouped data\n",
    "    df_content_matrix = df_content_sum[['parentid']].drop_duplicates()\n",
    "    subtype_list = df_content_sum['subtype'].unique()\n",
    "    subtype_list.sort()\n",
    "    for subtype in subtype_list:\n",
    "        if subtype == 'Document':\n",
    "            df = df_content_sum[['parentid', 'dataid', 'DATAID_ZMASTER', 'DATAID_CDMS', 'DATAID_JDE', 'DATAID_MAX', 'versionid', 'datasize']][(df_content_sum['subtype']) == subtype]\n",
    "            df.rename({'dataid': subtype, 'DATAID_ZMASTER': subtype+'_ZTAB', 'DATAID_CDMS': subtype+'_CDMS', 'DATAID_JDE': subtype+'_JDE', 'DATAID_MAX': subtype+'_MAX', 'versionid': subtype+'_VERS', 'datasize': subtype+'_GB'}, axis=1, inplace=True)\n",
    "        else:\n",
    "            df = df_content_sum[['parentid', 'dataid', 'DATAID_ZMASTER', 'DATAID_CDMS', 'DATAID_JDE', 'DATAID_MAX']][(df_content_sum['subtype']) == subtype]\n",
    "            df.rename({'dataid': subtype, 'DATAID_ZMASTER': subtype+'_ZTAB', 'DATAID_CDMS': subtype+'_CDMS', 'DATAID_JDE': subtype+'_JDE', 'DATAID_MAX': subtype+'_MAX'}, axis=1, inplace=True)\n",
    "        df_content_matrix = df_content_matrix.merge(df, how='left', on='parentid')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Transversed grouped content = {df_content_matrix.shape[0]} rows')\n",
    "\n",
    "\n",
    "    # 3. Generate final report per Root Folder\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Generating Final Report...')\n",
    "\n",
    "    # Merge CONTENTS with CONTAINERS\n",
    "    df_extract['cve_containers'] = df_extract['cve_containers'].merge(df_content_matrix, how='left', left_on='dataid', right_on='parentid', suffixes=('_FOL', '_DOC'))\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Merged Content & Containers = {df_extract[\"cve_containers\"].shape[0]} rows')\n",
    "\n",
    "    # Roll-up to Level 4\n",
    "    df_extract['cve_containers'].sort_values(by='path', inplace=True, na_position='first')\n",
    "    df_extract['cve_containers'].drop(['dataid', 'path', 'parentid'], axis=1, inplace=True)\n",
    "    df_report[root_folder] = df_extract['cve_containers'].groupby(by=['rootfolder', 'level2', 'level3', 'level4'], dropna=False).sum()\n",
    "    df_report[root_folder].sort_values(by=['rootfolder', 'level2', 'level3', 'level4'], inplace=True, na_position='first')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Rolled-up to Level 4 = {df_report[root_folder].shape[0]} rows')\n",
    "\n",
    "    # Report output\n",
    "    df_report[root_folder].to_excel(path + f'CVE - Data Analysis - {root_folder}.xlsx')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Report Output = {df_report[root_folder].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_create_oi(df: pd.DataFrame, type: str, output_file: str, sort_list: list, legacy_id: str, category_name: str):\n",
    "def build_create_oi(df: pd.DataFrame, type: str, output_file: str, sort_list: list, legacy_id: str, category_name: str, cat_atts: list):\n",
    "    for batch in df['batch'][(df['batch'].notnull())].sort_values().unique():\n",
    "        LegacyID_ant = ''\n",
    "        xml_file_name = 'create-' + type + '-' + output_file + '-' + str(batch).zfill(2) + '.xml'\n",
    "        df_batch = df[(df['batch'] == batch)].sort_values(by=sort_list)\n",
    "        root = ET.Element('import')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for key, value in df_batch.iterrows():\n",
    "            node = ET.Element('node')\n",
    "            node.set('type', type)\n",
    "            location = ET.SubElement(node, 'location')\n",
    "            location.text = value['location']\n",
    "            title = ET.SubElement(node, 'title')\n",
    "            title.text = value['title']\n",
    "            if type in ['document', 'email', 'caddocument']:\n",
    "                ET.SubElement(node, 'description').text = value['description']\n",
    "                ET.SubElement(node, 'created').text = value['created']\n",
    "                ET.SubElement(node, 'createdby').text = value['createdby']\n",
    "                ET.SubElement(node, 'modified').text = value['modified']\n",
    "                ET.SubElement(node, 'file').text = value['file']\n",
    "                ET.SubElement(node, 'filename').text = value['filename']\n",
    "                ET.SubElement(node, 'mime').text = value['mime']                    \n",
    "                if value[legacy_id] == LegacyID_ant:\n",
    "                    node.set('action', 'addversion')\n",
    "                    node.remove(location)\n",
    "                    node.remove(title)\n",
    "                    ET.SubElement(node, 'location').text = value['location'] + ':' + value['title']\n",
    "                else:\n",
    "                    node.set('action', 'create')\n",
    "                LegacyID_ant = value[legacy_id]\n",
    "            if type == 'url':\n",
    "                node.set('action', 'create')\n",
    "                ET.SubElement(node, 'url').text = value['url']\n",
    "            if type == 'folder':\n",
    "                node.set('action', 'create')\n",
    "            if type == 'alias':\n",
    "                node.set('action', 'create')\n",
    "                ET.SubElement(node, 'alias').text = value['alias']\n",
    "            if type == 'compounddoc':\n",
    "                node.set('action', 'create')\n",
    "            if len(cat_atts) != 0 and category_name != '':\n",
    "                category = ET.Element('category')\n",
    "                category.set('name', category_name)\n",
    "                for att in cat_atts:\n",
    "                    if len(value[att]) > 0: ET.SubElement(category, 'attribute', attrib={'name': att}).text = value[att]\n",
    "                node.append(category)\n",
    "            root.append(node)\n",
    "        xml = ET.tostring(root, encoding='utf-8', method='xml')\n",
    "        xml_parsed = dom.parseString(xml).toprettyxml()\n",
    "        xml_file = open(path + xml_path + xml_file_name, 'w',  encoding='utf-8')\n",
    "        xml_file.write(xml_parsed)\n",
    "        xml_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {xml_file_name} = {len(root)} nodes out of {df_batch.shape[0]}')\n",
    "        root.clear()\n",
    "        \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_update_cats_oi(df: pd.DataFrame, type: str, output_file: str, sort_list: list, category_name: str, cat_atts: list):\n",
    "    for batch in df['batch'][(df['batch'].notnull())].sort_values().unique():\n",
    "        LegacyID_ant = ''\n",
    "        xml_file_name = 'update-' + type + '-' + output_file + '-' + str(batch).zfill(2) + '.xml'\n",
    "        df_batch = df[(df['batch'] == batch)].sort_values(by=sort_list)\n",
    "        root = ET.Element('import')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for key, value in df_batch.iterrows():\n",
    "            node = ET.Element('node')\n",
    "            node.set('type', type)\n",
    "            node.set('action', 'update')\n",
    "            ET.SubElement(node, 'location').text = value['location'] + ':' + value['title']\n",
    "#            ET.SubElement(node, 'location').text = value['location']\n",
    "#            ET.SubElement(node, 'description').text = value['description']\n",
    "            if len(cat_atts) != 0 and category_name != '':\n",
    "                category = ET.Element('category')\n",
    "                category.set('name', category_name)\n",
    "                for att in cat_atts:\n",
    "                    if len(value[att]) > 0:\n",
    "#                        print(f'{value[att]} {len(value[att])}')\n",
    "                        ET.SubElement(category, 'attribute', attrib={'name': att}).text = value[att]\n",
    "                node.append(category)\n",
    "            root.append(node)\n",
    "        xml = ET.tostring(root, encoding='utf-8', method='xml')\n",
    "        xml_parsed = dom.parseString(xml).toprettyxml()\n",
    "        xml_file = open(path + xml_path + xml_file_name, 'w',  encoding='utf-8')\n",
    "        xml_file.write(xml_parsed)\n",
    "        xml_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {xml_file_name} = {len(root)} nodes out of {df_batch.shape[0]}')\n",
    "        root.clear()\n",
    "        \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_delete_oi(df: pd.DataFrame, type: str, output_file: str, sort_list: list):\n",
    "    for batch in df['batch'][(df['batch'].notnull())].sort_values().unique():\n",
    "        xml_file_name = 'delete-' + type + '-' + output_file + '-' + str(batch).zfill(2) + '.xml'\n",
    "        df_batch = df[(df['batch'] == batch)].sort_values(by=sort_list, ascending=False)\n",
    "        root = ET.Element('import')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for key, value in df_batch.iterrows():\n",
    "            node = ET.Element('node')\n",
    "            node.set('type', type)\n",
    "            node.set('action', 'delete')\n",
    "            ET.SubElement(node, 'location').text = value['location'] + ':' + value['title']\n",
    "#            ET.SubElement(node, 'title').text = value['title']\n",
    "            root.append(node)\n",
    "        xml = ET.tostring(root, encoding='utf-8', method='xml')\n",
    "        xml_parsed = dom.parseString(xml).toprettyxml()\n",
    "        xml_file = open(path + xml_path + xml_file_name, 'w',  encoding='utf-8')\n",
    "        xml_file.write(xml_parsed)\n",
    "        xml_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {xml_file_name} = {len(root)} nodes out of {df_batch.shape[0]}')\n",
    "        root.clear()\n",
    "        \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d775ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 25 2023 13:57:13 PM: Done\n"
     ]
    }
   ],
   "source": [
    "def release_extract_memory(names:list):\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Releasing Memory...')\n",
    "    \n",
    "    total_memory_used = 0\n",
    "    memory_freed = 0\n",
    "    for name in names:\n",
    "        # Calculate used memory\n",
    "        total_memory_used =+ df_extract[name].memory_usage().sum()\n",
    "    total_memory_used = total_memory_used/1024/1024\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Usage = {total_memory_used} MB')\n",
    "\n",
    "    # Delete DataFrames\n",
    "    for name in names:\n",
    "        del df_extract[name]\n",
    "        df_extract[name] = pd.DataFrame()\n",
    "        memory_freed =+ gc.collect()\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Memory Freed = {memory_freed}')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108ecd3",
   "metadata": {},
   "source": [
    "# 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95866af3",
   "metadata": {},
   "source": [
    "### 2.1 Prepare & Analyze JDE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862fd88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# JDE Files Analysis\n",
    "\n",
    "for file in JDE_files.keys():\n",
    "    # Upload JDE Data\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading JDE data...')\n",
    "    df_extract[file] = pd.read_csv(path + JDE_files[file])\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loaded JDE {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "    # Calculate Server link\n",
    "    df_extract[file]['server_link'] = df_extract[file].apply(lambda x: x['url'][0:x['url'].find('/', 8)], axis=1)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculated Server Link {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "    # Extract DataIds List\n",
    "    # http://livelink, http://livelink.encana.com, https://contentserver.cenovus.com, http://doc, http://doc.encana.com\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Extracting candidate DataIds...')\n",
    "    df_extract[file]['cve_dataid_list'] = df_extract[file].apply(lambda x: None if x['server_link'] != 'http://livelink' and x['server_link'] != 'http://livelink.encana.com' and x['server_link'] != 'https://contentserver.cenovus.com' and x['server_link'] != 'http://doc' and x['server_link'] != 'http://doc.encana.com'\n",
    "                                                    else (re.findall('(?<=docref=)[0-9]*', x['url']) if x['url'].find('docref=') != -1 \n",
    "                                                          else (re.findall('(?<=docid=)[0-9]*', x['url']) if x['url'].find('docid=') != -1\n",
    "                                                                else (re.findall('(?<=nodeid=)[0-9]*', x['url']) if x['url'].find('nodeid=') != -1\n",
    "                                                                      else (re.findall('(?<=nodeId=)[0-9]*', x['url']) if x['url'].find('nodeId=') != -1\n",
    "                                                                            else (re.findall('(?<=\\/open/)[0-9]*', x['url']) if x['url'].find('/open/') != -1\n",
    "                                                                                  else (re.findall('(?<=\\/Open/)[0-9]*', x['url']) if x['url'].find('/Open/') != -1\n",
    "                                                                                        else (re.findall('(?<=objId=)[0-9]*', x['url']) if x['url'].find('objId=') != -1\n",
    "                                                                                              else (re.findall('(?<=objid=)[0-9]*', x['url']) if x['url'].find('objid=') != -1\n",
    "                                                                                                    else (re.findall('(?<=\\/nodes/)[0-9]*', x['url']) if x['url'].find('/nodes/') != -1\n",
    "                                                                                                          else 'no_value_found'))))))))), axis=1)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Extracted DataIds for {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "    # Extract DataIds\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Converting candidate DataIds to numeric type...')\n",
    "    df_extract[file]['cve_dataid'] = df_extract[file][['cve_dataid_list', 'server_link']].apply(lambda x: None if x['server_link'] != 'http://livelink' and x['server_link'] != 'http://livelink.encana.com' and x['server_link'] != 'https://contentserver.cenovus.com' and x['server_link'] != 'http://doc' and x['server_link'] != 'http://doc.encana.com' \n",
    "                                                                                                else (0 if len(x['cve_dataid_list']) == 0\n",
    "                                                                                                      else (x['cve_dataid_list'][0] if x['cve_dataid_list'] != 'no_value_found'\n",
    "                                                                                                            else x['cve_dataid_list'])), axis=1)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Converted DataIds for {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "    # Calculate Type\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculating Type...')\n",
    "    df_extract[file]['type'] = df_extract[file][['cve_dataid_list', 'server_link']].apply(lambda x: 'not cs' if x['server_link'] != 'http://livelink' and x['server_link'] != 'http://livelink.encana.com' and x['server_link'] != 'https://contentserver.cenovus.com' and x['server_link'] != 'http://doc' and x['server_link'] != 'http://doc.encana.com' \n",
    "                                                                                          else ('no_value' if len(x['cve_dataid_list']) == 0\n",
    "                                                                                                else ('candidate_value' if x['cve_dataid_list'] != 'no_value_found'\n",
    "                                                                                                      else x['cve_dataid_list'])), axis=1)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculated Type for {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "    # Remove cve_dataid_list\n",
    "    if 'cve_dataid_list' in df_extract[file].columns: df_extract[file].drop(['cve_dataid_list'], axis=1, inplace=True)\n",
    "\n",
    "    # Reports for JDE links\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Creating JDE extracts...')\n",
    "    # Counts\n",
    "    df_extract[file].sort_values(by=['server_link', 'url'], inplace=True)\n",
    "    df_extract[file].groupby(by=['server_link', 'type']).agg({'url': [pd.Series.count, pd.Series.nunique]}).to_excel(path + f'CVE - Data Analysis - JDE - {file} Summary.xlsx')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Count Links per Server {file} = {df_extract[file].shape[0]} rows')\n",
    "    # All links Translation\n",
    "    df_extract[file].to_csv(extract_path + f'CVE - Data Analysis - JDE - {file} Translated links.csv')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Saving Links translation {file} = {df_extract[file].shape[0]} rows')\n",
    "    # Scorpio and ECN slices\n",
    "    df_extract[file][(df_extract[file]['server_link']) == 'http://scorpio'].to_csv(extract_path + f'CVE - Data Analysis - JDE - {file} Scorpio links.csv', index=False)\n",
    "    df_extract[file][(df_extract[file]['server_link']) == 'http://ecn.encana.com'].to_csv(extract_path + f'CVE - Data Analysis - JDE - {file} Ecn links.csv', index=False)\n",
    "    # Save DataIds to file\n",
    "    pd.DataFrame(df_extract[file]['cve_dataid'][(df_extract[file]['type'] == 'candidate_value')].unique(), columns=['DATAID_JDE']).to_csv(extract_path + f'DataIds_JDE_{file}.csv', index=False)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Saved DataIds to file {file} = {df_extract[file].shape[0]} rows')\n",
    "    \n",
    "    # Release Memory\n",
    "    release_extract_memory([file])\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Memory released {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f9ed4",
   "metadata": {},
   "source": [
    "### 2.2 Prepare and Analyze MAXIMO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66b236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAXIMO Files Analysis\n",
    "\n",
    "# Upload MAXIMO Data\n",
    "df_extract['maximo'] = pd.DataFrame()\n",
    "for file in MAXIMO_files.keys():\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading MAXIMO data...')\n",
    "    df_extract[file] = pd.read_csv(path + MAXIMO_files[file], encoding_errors='replace')\n",
    "    if df_extract['maximo'].empty:\n",
    "        df_extract['maximo'] = df_extract[file]\n",
    "    else:\n",
    "        df_extract['maximo'] = pd.concat([df_extract['maximo'], df_extract[file]])\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loaded {file} = {df_extract[file].shape[0]} rows')\n",
    "df_extract['maximo'].rename(columns={'URLNAME':'url'}, inplace=True)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loaded all maximo files = {df_extract[\"maximo\"].shape[0]} rows')\n",
    "\n",
    "\n",
    "file = 'maximo'\n",
    "# Calculate Server link\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculating server link...')\n",
    "df_extract[file]['server_link'] = df_extract[file].apply(lambda x: x['url'][0:x['url'].find('/', 8)], axis=1)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculated Server Link {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "# Extract DataIds List\n",
    "# http://livelink, http://livelink.encana.com, https://contentserver.cenovus.com, http://doc, http://doc.encana.com\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Extracting candidate DataIds...')\n",
    "df_extract[file]['cve_dataid_list'] = df_extract[file].apply(lambda x: None if x['server_link'].find('livelink') == -1 and x['server_link'].find('contentserver') == -1 and x['server_link'].find('http://doc') == -1\n",
    "                                                else (re.findall('(?<=docref=)[0-9]*', x['url']) if x['url'].find('docref=') != -1 \n",
    "                                                      else (re.findall('(?<=docid=)[0-9]*', x['url']) if x['url'].find('docid=') != -1\n",
    "                                                            else (re.findall('(?<=nodeid=)[0-9]*', x['url']) if x['url'].find('nodeid=') != -1\n",
    "                                                                  else (re.findall('(?<=nodeId=)[0-9]*', x['url']) if x['url'].find('nodeId=') != -1\n",
    "                                                                        else (re.findall('(?<=\\/open/)[0-9]*', x['url']) if x['url'].find('/open/') != -1\n",
    "                                                                              else (re.findall('(?<=\\/Open/)[0-9]*', x['url']) if x['url'].find('/Open/') != -1\n",
    "                                                                                    else (re.findall('(?<=objId=)[0-9]*', x['url']) if x['url'].find('objId=') != -1\n",
    "                                                                                          else (re.findall('(?<=objid=)[0-9]*', x['url']) if x['url'].find('objid=') != -1\n",
    "                                                                                                else (re.findall('(?<=\\/nodes/)[0-9]*', x['url']) if x['url'].find('/nodes/') != -1\n",
    "                                                                                                      else 'no_value_found'))))))))), axis=1)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Extracted DataIds for {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "# Extract DataIds\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Converting candidate DataIds to numeric type...')\n",
    "df_extract[file]['cve_dataid'] = df_extract[file][['cve_dataid_list', 'server_link']].apply(lambda x: None if x['server_link'].find('livelink') == -1 and x['server_link'].find('contentserver') == -1 and x['server_link'].find('http://doc') == -1\n",
    "                                                                                            else (0 if len(x['cve_dataid_list']) == 0\n",
    "                                                                                                  else (x['cve_dataid_list'][0] if x['cve_dataid_list'] != 'no_value_found'\n",
    "                                                                                                        else x['cve_dataid_list'])), axis=1)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Converted DataIds for {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "# Calculate Type\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculating Type...')\n",
    "df_extract[file]['type'] = df_extract[file][['cve_dataid_list', 'server_link']].apply(lambda x: 'not cs' if x['server_link'].find('livelink') == -1 and x['server_link'].find('contentserver') == -1 and x['server_link'].find('http://doc') == -1\n",
    "                                                                                      else ('no_value' if len(x['cve_dataid_list']) == 0\n",
    "                                                                                            else ('candidate_value' if x['cve_dataid_list'] != 'no_value_found'\n",
    "                                                                                                  else x['cve_dataid_list'])), axis=1)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculated Type for {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "# Remove cve_dataid_list\n",
    "if 'cve_dataid_list' in df_extract[file].columns: df_extract[file].drop(['cve_dataid_list'], axis=1, inplace=True)\n",
    "\n",
    "# Reports for MAXIMO links\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Creating MAXIMO extracts...')\n",
    "# Counts\n",
    "df_extract[file].sort_values(by=['server_link', 'url'], inplace=True)\n",
    "df_extract[file].groupby(by=['server_link', 'type']).agg({'url': [pd.Series.count, pd.Series.nunique]}).to_excel(path + f'CVE - Data Analysis - MAX - {file} Summary.xlsx')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Count Links per Server {file} = {df_extract[file].shape[0]} rows')\n",
    "# All links Translation\n",
    "df_extract[file].to_csv(extract_path + f'CVE - Data Analysis - MAX - {file} Translated links.csv')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Saving Links translation {file} = {df_extract[file].shape[0]} rows')\n",
    "# Save DataIds to file\n",
    "pd.DataFrame(df_extract[file]['cve_dataid'][(df_extract[file]['type'] == 'candidate_value')].unique(), columns=['DATAID_MAX']).to_csv(extract_path + f'DataIds_MAX_{file}.csv', index=False)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Saved DataIds to file {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "# Release Memory\n",
    "release_extract_memory([file])\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Memory released {file} = {df_extract[file].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a857b4",
   "metadata": {},
   "source": [
    "### 2.3 Prepare CDMS Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ad25d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload CDMS Data\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading CDMS data...')\n",
    "cnxn = sql.connect(cnxn_str['cdms'])\n",
    "df_extract['cdms'] = pd.read_sql(query['cdms'], cnxn)\n",
    "cnxn.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loaded CDMS= {df_extract[\"cdms\"].shape[0]} rows')\n",
    "\n",
    "# Change DataTypes to match merge \n",
    "#df_extract['cdms'] = df_extract['cdms'].astype({'DATAID_CVE': 'int64'}, copy='False')\n",
    "\n",
    "# Save DataIds to file\n",
    "df_extract['cdms'].to_csv(extract_path + f'DataIds_CDMS.csv', index=False)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Saved DataIds to file = {df_extract[\"cdms\"].shape[0]} rows')\n",
    "\n",
    "# Release Memory\n",
    "release_extract_memory(['cdms'])\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Memory released = {df_extract[\"cdms\"].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdf30d",
   "metadata": {},
   "source": [
    "### 2.4 Prepare Z_MIGRATION_MASTER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345d447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload Z_MIGRATION_MASTER Data\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading Z_MIGRATION_MASTER data...')\n",
    "cnxn = ora_engine.connect()\n",
    "df_extract['zmaster'] = pd.read_sql(query['zmaster'], cnxn)\n",
    "cnxn.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loaded Z_MIGRATION_MASTER = {df_extract[\"zmaster\"].shape[0]} rows')\n",
    "\n",
    "# Change DataTypes to match merge \n",
    "#df_extract['cdms'] = df_extract['cdms'].astype({'DATAID_CVE': 'int64'}, copy='False')\n",
    "\n",
    "# Save DataIds to file\n",
    "df_extract['zmaster'].drop_duplicates(inplace=True)\n",
    "df_extract['zmaster'].rename(columns={'olddataid': 'DATAID_ZMASTER'}, inplace=True)\n",
    "df_extract['zmaster'].to_csv(extract_path + f'DataIds_ZMASTER.csv', index=False)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Saved DataIds to file = {df_extract[\"zmaster\"].shape[0]} rows')\n",
    "\n",
    "# Release Memory\n",
    "release_extract_memory(['zmaster'])\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Memory released = {df_extract[\"zmaster\"].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac66ad5",
   "metadata": {},
   "source": [
    "# 3. Data Assessment & Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07da98",
   "metadata": {},
   "source": [
    "### 3.1 Upload CDMS, JDE, Maximo DataIds flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ca59a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 25 2023 14:26:12 PM: Uploaded JDE DataIds = 14201637 rows\n",
      "Sep 25 2023 14:26:13 PM: Uploaded MAXIMO DataIds = 979353 rows\n",
      "Sep 25 2023 14:26:56 PM: Uploaded CDMS DataIds = 19146903 rows\n",
      "Sep 25 2023 14:27:14 PM: Uploaded Z_MIGRATION_MASTER DataIds = 16348452 rows\n",
      "Sep 25 2023 14:27:14 PM: Done\n"
     ]
    }
   ],
   "source": [
    "# Upload JDE DataIds from file\n",
    "df_extract['JDE'] = pd.concat([pd.read_csv(extract_path + 'DataIds_JDE_jdepddta.csv'), pd.read_csv(extract_path + 'DataIds_JDE_jdelgdta.csv')]).drop_duplicates()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Uploaded JDE DataIds = {df_extract[\"JDE\"].shape[0]} rows')\n",
    "\n",
    "# Upload MAXIMO DataIds from file\n",
    "df_extract['MAXIMO'] = pd.read_csv(extract_path + 'DataIds_MAX_maximo.csv').drop_duplicates()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Uploaded MAXIMO DataIds = {df_extract[\"MAXIMO\"].shape[0]} rows')\n",
    "\n",
    "# Upload CDMS DataIds from file\n",
    "df_extract['CDMS'] = pd.read_csv(extract_path + 'DataIds_CDMS.csv').drop_duplicates()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Uploaded CDMS DataIds = {df_extract[\"CDMS\"].shape[0]} rows')\n",
    "\n",
    "# Upload ZMASTER DataIds from file\n",
    "df_extract['ZMASTER'] = pd.read_csv(extract_path + 'DataIds_ZMASTER.csv').drop_duplicates()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Uploaded Z_MIGRATION_MASTER DataIds = {df_extract[\"ZMASTER\"].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75807210",
   "metadata": {},
   "source": [
    "### 3.2 Upload Folders & Contents Data and create Reports per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "351088c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 26 2023 06:08:37 AM: Loading root folders...\n",
      "Sep 26 2023 06:08:37 AM:     Loaded cve_root_folders = 32 rows\n",
      "Sep 26 2023 06:08:37 AM: ********** FOLDER = Application Data **********\n",
      "Sep 26 2023 06:08:37 AM: Loading CVE Folder and Document data...\n",
      "Sep 26 2023 06:14:31 AM:     Loaded cve_containers= 513953 rows\n",
      "Sep 26 2023 06:30:05 AM:     Loaded cve_contents= 2746760 rows\n",
      "Sep 26 2023 06:30:05 AM: Merging CDMS...\n",
      "Sep 26 2023 06:30:29 AM:     Merged CDMS = 2746760 rows\n",
      "Sep 26 2023 06:30:29 AM: Merging ZMASTER...\n",
      "Sep 26 2023 06:30:56 AM:     Merged ZMASTER = 2746760 rows\n",
      "Sep 26 2023 06:30:56 AM: Merging JDE...\n",
      "Sep 26 2023 06:31:33 AM:     Merged JDE = 2746760 rows\n",
      "Sep 26 2023 06:31:33 AM: Merging MAXIMO...\n",
      "Sep 26 2023 06:31:38 AM:     Merged MAXIMO = 2746760 rows\n",
      "Sep 26 2023 06:31:38 AM: Grouping per ParentID...\n",
      "Sep 26 2023 06:45:48 AM:     Grouped cve_contents = 512741 rows\n",
      "Sep 26 2023 06:45:51 AM:     Transversed grouped content = 512737 rows\n",
      "Sep 26 2023 06:45:51 AM: Generating Final Report...\n",
      "Sep 26 2023 06:45:53 AM:     Merged Content & Containers = 513953 rows\n",
      "Sep 26 2023 06:45:55 AM:     Rolled-up to Level 4 = 172 rows\n",
      "Sep 26 2023 06:45:55 AM:     Report Output = 172 rows\n",
      "Sep 26 2023 06:45:55 AM: Releasing Memory...\n",
      "Sep 26 2023 06:45:55 AM:     Memory Usage = 251.473388671875 MB\n",
      "Sep 26 2023 06:45:56 AM:     Memory Freed = 0\n"
     ]
    }
   ],
   "source": [
    "# ***** ALL Root Folders except Upstream Operations *****\n",
    "\n",
    "#(7 hrs 20 min)\n",
    "# Create list of extract Folders: Root Folders\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading root folders...')\n",
    "cnxn = ora_engine.connect()\n",
    "df_extract['cve_folders'] = pd.read_sql(query['cve_root_folders'], cnxn)\n",
    "cnxn.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded cve_root_folders = {df_extract[\"cve_folders\"].shape[0]} rows')\n",
    "\n",
    "#Remove Upstream Operations\n",
    "df_extract['cve_folders'].drop(axis=0, index=[30], inplace=True)\n",
    "\n",
    "# Build Data Assessment Report\n",
    "#for idx, row in df_extract['cve_folders'][(df_extract['cve_folders'].index == 0)].iterrows():\n",
    "for idx, row in df_extract['cve_folders'].iterrows():\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: ********** FOLDER = {row[\"name\"]} **********')\n",
    "    # Upload container and contents for root folder from Database\n",
    "    upload_cve_data(row['dataid'], save_to_file=False)\n",
    "    \n",
    "    # Merge all data and create report\n",
    "    create_data_assessment_report(row['name'])\n",
    "\n",
    "    # Release Memory\n",
    "    release_extract_memory(['cve_containers', 'cve_contents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ea6cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 23 2023 06:14:31 AM: ********** FOLDER = Canadian Plains_NOMIGRATE **********\n",
      "Sep 23 2023 06:14:31 AM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 06:15:22 AM:     Loaded cve_containers= 23282 rows\n",
      "Sep 23 2023 06:16:46 AM:     Loaded cve_contents= 293143 rows\n",
      "Sep 23 2023 06:16:46 AM: Merging CDMS...\n",
      "Sep 23 2023 06:17:09 AM:     Merged CDMS = 293143 rows\n",
      "Sep 23 2023 06:17:09 AM: Merging ZMASTER...\n",
      "Sep 23 2023 06:17:28 AM:     Merged ZMASTER = 293143 rows\n",
      "Sep 23 2023 06:17:28 AM: Merging JDE...\n",
      "Sep 23 2023 06:18:05 AM:     Merged JDE = 293143 rows\n",
      "Sep 23 2023 06:18:05 AM: Merging MAXIMO...\n",
      "Sep 23 2023 06:18:07 AM:     Merged MAXIMO = 293143 rows\n",
      "Sep 23 2023 06:18:07 AM: Grouping per ParentID...\n",
      "Sep 23 2023 06:18:59 AM:     Grouped cve_contents = 33008 rows\n",
      "Sep 23 2023 06:19:00 AM:     Transversed grouped content = 29722 rows\n",
      "Sep 23 2023 06:19:00 AM: Generating Final Report...\n",
      "Sep 23 2023 06:19:00 AM:     Merged Content & Containers = 23282 rows\n",
      "Sep 23 2023 06:19:00 AM:     Rolled-up to Level 4 = 785 rows\n",
      "Sep 23 2023 06:19:03 AM:     Report Output = 785 rows\n",
      "Sep 23 2023 06:19:03 AM: Releasing Memory...\n",
      "Sep 23 2023 06:19:03 AM:     Memory Usage = 26.838043212890625 MB\n",
      "Sep 23 2023 06:19:03 AM:     Memory Freed = 0\n",
      "Sep 23 2023 06:19:03 AM: ********** FOLDER = Christina Lake_NOMIGRATE **********\n",
      "Sep 23 2023 06:19:03 AM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 06:19:03 AM:     Loaded cve_containers= 45 rows\n",
      "Sep 23 2023 06:19:03 AM:     Loaded cve_contents= 740 rows\n",
      "Sep 23 2023 06:19:03 AM: Merging CDMS...\n",
      "Sep 23 2023 06:19:27 AM:     Merged CDMS = 740 rows\n",
      "Sep 23 2023 06:19:27 AM: Merging ZMASTER...\n",
      "Sep 23 2023 06:19:48 AM:     Merged ZMASTER = 740 rows\n",
      "Sep 23 2023 06:19:48 AM: Merging JDE...\n",
      "Sep 23 2023 06:20:20 AM:     Merged JDE = 740 rows\n",
      "Sep 23 2023 06:20:20 AM: Merging MAXIMO...\n",
      "Sep 23 2023 06:20:21 AM:     Merged MAXIMO = 740 rows\n",
      "Sep 23 2023 06:20:21 AM: Grouping per ParentID...\n",
      "Sep 23 2023 06:20:21 AM:     Grouped cve_contents = 53 rows\n",
      "Sep 23 2023 06:20:21 AM:     Transversed grouped content = 45 rows\n",
      "Sep 23 2023 06:20:21 AM: Generating Final Report...\n",
      "Sep 23 2023 06:20:21 AM:     Merged Content & Containers = 45 rows\n",
      "Sep 23 2023 06:20:21 AM:     Rolled-up to Level 4 = 43 rows\n",
      "Sep 23 2023 06:20:21 AM:     Report Output = 43 rows\n",
      "Sep 23 2023 06:20:21 AM: Releasing Memory...\n",
      "Sep 23 2023 06:20:21 AM:     Memory Usage = 0.0677490234375 MB\n",
      "Sep 23 2023 06:20:22 AM:     Memory Freed = 0\n",
      "Sep 23 2023 06:20:22 AM: ********** FOLDER = Cross-Region_NOMIGRATE **********\n",
      "Sep 23 2023 06:20:22 AM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 06:20:44 AM:     Loaded cve_containers= 2797 rows\n",
      "Sep 23 2023 06:21:07 AM:     Loaded cve_contents= 65760 rows\n",
      "Sep 23 2023 06:21:07 AM: Merging CDMS...\n",
      "Sep 23 2023 06:21:31 AM:     Merged CDMS = 65760 rows\n",
      "Sep 23 2023 06:21:31 AM: Merging ZMASTER...\n",
      "Sep 23 2023 06:21:48 AM:     Merged ZMASTER = 65760 rows\n",
      "Sep 23 2023 06:21:48 AM: Merging JDE...\n",
      "Sep 23 2023 06:22:25 AM:     Merged JDE = 65760 rows\n",
      "Sep 23 2023 06:22:25 AM: Merging MAXIMO...\n",
      "Sep 23 2023 06:22:26 AM:     Merged MAXIMO = 65760 rows\n",
      "Sep 23 2023 06:22:26 AM: Grouping per ParentID...\n",
      "Sep 23 2023 06:22:31 AM:     Grouped cve_contents = 2775 rows\n",
      "Sep 23 2023 06:22:31 AM:     Transversed grouped content = 2773 rows\n",
      "Sep 23 2023 06:22:31 AM: Generating Final Report...\n",
      "Sep 23 2023 06:22:31 AM:     Merged Content & Containers = 2797 rows\n",
      "Sep 23 2023 06:22:31 AM:     Rolled-up to Level 4 = 261 rows\n",
      "Sep 23 2023 06:22:31 AM:     Report Output = 261 rows\n",
      "Sep 23 2023 06:22:31 AM: Releasing Memory...\n",
      "Sep 23 2023 06:22:31 AM:     Memory Usage = 6.0205078125 MB\n",
      "Sep 23 2023 06:22:32 AM:     Memory Freed = 0\n",
      "Sep 23 2023 06:22:32 AM: ********** FOLDER = Finance **********\n",
      "Sep 23 2023 06:22:32 AM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 07:21:59 AM:     Loaded cve_containers= 7032955 rows\n",
      "Sep 23 2023 08:55:33 AM:     Loaded cve_contents= 25973211 rows\n",
      "Sep 23 2023 08:55:34 AM: Merging CDMS...\n",
      "Sep 23 2023 08:56:52 AM:     Merged CDMS = 25973211 rows\n",
      "Sep 23 2023 08:56:52 AM: Merging ZMASTER...\n",
      "Sep 23 2023 08:58:04 AM:     Merged ZMASTER = 25973211 rows\n",
      "Sep 23 2023 08:58:04 AM: Merging JDE...\n",
      "Sep 23 2023 08:59:41 AM:     Merged JDE = 25973211 rows\n",
      "Sep 23 2023 08:59:41 AM: Merging MAXIMO...\n",
      "Sep 23 2023 09:00:16 AM:     Merged MAXIMO = 25973211 rows\n",
      "Sep 23 2023 09:00:16 AM: Grouping per ParentID...\n",
      "Sep 23 2023 11:59:20 AM:     Grouped cve_contents = 7033671 rows\n",
      "Sep 23 2023 12:01:01 PM:     Transversed grouped content = 7032061 rows\n",
      "Sep 23 2023 12:01:01 PM: Generating Final Report...\n",
      "Sep 23 2023 12:02:05 PM:     Merged Content & Containers = 7032955 rows\n",
      "Sep 23 2023 12:02:40 PM:     Rolled-up to Level 4 = 106 rows\n",
      "Sep 23 2023 12:02:41 PM:     Report Output = 106 rows\n",
      "Sep 23 2023 12:02:42 PM: Releasing Memory...\n",
      "Sep 23 2023 12:02:42 PM:     Memory Usage = 2377.918487548828 MB\n",
      "Sep 23 2023 12:02:59 PM:     Memory Freed = 0\n",
      "Sep 23 2023 12:02:59 PM: ********** FOLDER = Foothills Region_NOMIGRATE **********\n",
      "Sep 23 2023 12:02:59 PM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 12:03:18 PM:     Loaded cve_containers= 9905 rows\n",
      "Sep 23 2023 12:03:57 PM:     Loaded cve_contents= 140922 rows\n",
      "Sep 23 2023 12:03:57 PM: Merging CDMS...\n",
      "Sep 23 2023 12:04:18 PM:     Merged CDMS = 140922 rows\n",
      "Sep 23 2023 12:04:18 PM: Merging ZMASTER...\n",
      "Sep 23 2023 12:04:38 PM:     Merged ZMASTER = 140922 rows\n",
      "Sep 23 2023 12:04:38 PM: Merging JDE...\n",
      "Sep 23 2023 12:05:09 PM:     Merged JDE = 140922 rows\n",
      "Sep 23 2023 12:05:09 PM: Merging MAXIMO...\n",
      "Sep 23 2023 12:05:10 PM:     Merged MAXIMO = 140922 rows\n",
      "Sep 23 2023 12:05:10 PM: Grouping per ParentID...\n",
      "Sep 23 2023 12:05:23 PM:     Grouped cve_contents = 14631 rows\n",
      "Sep 23 2023 12:05:23 PM:     Transversed grouped content = 13674 rows\n",
      "Sep 23 2023 12:05:23 PM: Generating Final Report...\n",
      "Sep 23 2023 12:05:23 PM:     Merged Content & Containers = 9905 rows\n",
      "Sep 23 2023 12:05:23 PM:     Rolled-up to Level 4 = 86 rows\n",
      "Sep 23 2023 12:05:23 PM:     Report Output = 86 rows\n",
      "Sep 23 2023 12:05:23 PM: Releasing Memory...\n",
      "Sep 23 2023 12:05:23 PM:     Memory Usage = 12.90179443359375 MB\n",
      "Sep 23 2023 12:05:23 PM:     Memory Freed = 0\n",
      "Sep 23 2023 12:05:23 PM: ********** FOLDER = Foster Creek **********\n",
      "Sep 23 2023 12:05:23 PM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 12:05:57 PM:     Loaded cve_containers= 91298 rows\n",
      "Sep 23 2023 12:06:40 PM:     Loaded cve_contents= 230663 rows\n",
      "Sep 23 2023 12:06:40 PM: Merging CDMS...\n",
      "Sep 23 2023 12:07:01 PM:     Merged CDMS = 230663 rows\n",
      "Sep 23 2023 12:07:01 PM: Merging ZMASTER...\n",
      "Sep 23 2023 12:07:19 PM:     Merged ZMASTER = 230663 rows\n",
      "Sep 23 2023 12:07:19 PM: Merging JDE...\n",
      "Sep 23 2023 12:07:48 PM:     Merged JDE = 230663 rows\n",
      "Sep 23 2023 12:07:48 PM: Merging MAXIMO...\n",
      "Sep 23 2023 12:07:49 PM:     Merged MAXIMO = 230663 rows\n",
      "Sep 23 2023 12:07:49 PM: Grouping per ParentID...\n",
      "Sep 23 2023 12:08:33 PM:     Grouped cve_contents = 37138 rows\n",
      "Sep 23 2023 12:08:33 PM:     Transversed grouped content = 34678 rows\n",
      "Sep 23 2023 12:08:33 PM: Generating Final Report...\n",
      "Sep 23 2023 12:08:33 PM:     Merged Content & Containers = 91298 rows\n",
      "Sep 23 2023 12:08:34 PM:     Rolled-up to Level 4 = 1412 rows\n",
      "Sep 23 2023 12:08:35 PM:     Report Output = 1412 rows\n",
      "Sep 23 2023 12:08:35 PM: Releasing Memory...\n",
      "Sep 23 2023 12:08:35 PM:     Memory Usage = 21.117828369140625 MB\n",
      "Sep 23 2023 12:08:36 PM:     Memory Freed = 0\n",
      "Sep 23 2023 12:08:36 PM: ********** FOLDER = Upstream Business Services **********\n",
      "Sep 23 2023 12:08:36 PM: Loading CVE Folder and Document data...\n",
      "Sep 23 2023 12:23:05 PM:     Loaded cve_containers= 839941 rows\n",
      "Sep 23 2023 12:53:19 PM:     Loaded cve_contents= 5800605 rows\n",
      "Sep 23 2023 12:53:19 PM: Merging CDMS...\n",
      "Sep 23 2023 12:53:47 PM:     Merged CDMS = 5800605 rows\n",
      "Sep 23 2023 12:53:47 PM: Merging ZMASTER...\n",
      "Sep 23 2023 12:54:13 PM:     Merged ZMASTER = 5800605 rows\n",
      "Sep 23 2023 12:54:13 PM: Merging JDE...\n",
      "Sep 23 2023 12:54:58 PM:     Merged JDE = 5800605 rows\n",
      "Sep 23 2023 12:54:58 PM: Merging MAXIMO...\n",
      "Sep 23 2023 12:55:06 PM:     Merged MAXIMO = 5800605 rows\n",
      "Sep 23 2023 12:55:06 PM: Grouping per ParentID...\n",
      "Sep 23 2023 13:18:20 PM:     Grouped cve_contents = 868430 rows\n",
      "Sep 23 2023 13:18:32 PM:     Transversed grouped content = 760376 rows\n",
      "Sep 23 2023 13:18:32 PM: Generating Final Report...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 23 2023 13:18:35 PM:     Merged Content & Containers = 839941 rows\n",
      "Sep 23 2023 13:18:40 PM:     Rolled-up to Level 4 = 850 rows\n",
      "Sep 23 2023 13:18:42 PM:     Report Output = 850 rows\n",
      "Sep 23 2023 13:18:42 PM: Releasing Memory...\n",
      "Sep 23 2023 13:18:42 PM:     Memory Usage = 531.0612487792969 MB\n",
      "Sep 23 2023 13:18:44 PM:     Memory Freed = 0\n"
     ]
    }
   ],
   "source": [
    "# ***** Upstream Operations *****\n",
    "\n",
    "#(7 hrs)\n",
    "# Create list of extract Folders: Upstream Operations Subfolders\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading CVE subfolders...')\n",
    "query['cve_folders'] = \"\"\"\n",
    "        SELECT  DataId, Name\n",
    "        FROM    dtree\n",
    "        WHERE   parentid = 4673\n",
    "        AND     subtype = 0\n",
    "        ORDER BY Name\n",
    "        \"\"\"\n",
    "cnxn = ora_engine.connect()\n",
    "df_extract['cve_folders'] = pd.read_sql(query['cve_folders'], cnxn)\n",
    "cnxn.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded cve_folders = {df_extract[\"cve_folders\"].shape[0]} rows')\n",
    "\n",
    "# Build Data Assessment Report\n",
    "#for idx, row in df_extract['cve_folders'][(df_extract['cve_folders'].index == 31)].iterrows():\n",
    "for idx, row in df_extract['cve_folders'].iterrows():\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: ********** FOLDER = {row[\"name\"]} **********')\n",
    "    # Upload container and contents for root folder from Database\n",
    "    upload_cve_data(row['dataid'], save_to_file=False)\n",
    "    \n",
    "    # Merge all data and create report\n",
    "    create_data_assessment_report(row['name'])\n",
    "\n",
    "    # Release Memory\n",
    "    release_extract_memory(['cve_containers', 'cve_contents'])\n",
    "\n",
    "# Consolidate Upstream Operations subfolders\n",
    "#Rollup subfolders to Level 3\n",
    "for idx, row in df_extract['cve_folders'].iterrows():\n",
    "    root_folder = row['name']\n",
    "    df_report[root_folder].sort_values(by=['rootfolder', 'level2', 'level3', 'level4'], inplace=True, na_position='first')\n",
    "    df_report[root_folder + '_l3'] = df_report[root_folder].groupby(by=['rootfolder', 'level2', 'level3'], dropna=False).sum()\n",
    "    df_report[root_folder + '_l3'].sort_values(by=['rootfolder', 'level2', 'level3'], inplace=True, na_position='first')\n",
    "    df_report[root_folder + '_l3'].to_excel(path + f'CVE - Data Analysis - {root_folder + \"_l3\"}.xlsx')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Rolled-up to Level 3 = {df_report[root_folder + \"_l3\"].shape[0]} rows')\n",
    "#Concatenate all level 3 subfolders\n",
    "root_folder = 'Upstream Operations'\n",
    "df_report[root_folder] = pd.concat([df_report['Canadian Plains_NOMIGRATE_l3'],\n",
    "                                    df_report['Christina Lake_NOMIGRATE_l3'],\n",
    "                                    df_report['Cross-Region_NOMIGRATE'],\n",
    "                                    df_report['Finance_l3'],\n",
    "                                    df_report['Foothills Region_NOMIGRATE_l3'],\n",
    "                                    df_report['Foster Creek_l3'],\n",
    "                                    df_report['Upstream Business Services_l3']])\n",
    "\n",
    "# Report output\n",
    "df_report[root_folder].to_excel(path + f'CVE - Data Analysis - {root_folder}.xlsx')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Report Output = {df_report[root_folder].shape[0]} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df0b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examples to roll-up to higher folder level\n",
    "\n",
    "# Roll-up to Root Level\n",
    "root_folder = 'Oil Sands'\n",
    "df_report[root_folder].sort_values(by=['rootfolder', 'level2', 'level3', 'level4'], inplace=True, na_position='first')\n",
    "df_report[root_folder + '_l1'] = df_report[root_folder].groupby(by=['rootfolder'], dropna=False).sum()\n",
    "df_report[root_folder + '_l1'].sort_values(by=['rootfolder'], inplace=True, na_position='first')\n",
    "df_report[root_folder + '_l1'].to_excel(path + f'CVE - Data Analysis - {root_folder + \"_l1\"}.xlsx')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Rolled-up to Level 1 = {df_report[root_folder + \"_l1\"].shape[0]} rows')\n",
    "\n",
    "#df_report[root_folder+'_l1'] = df_extract['cve_containers'].groupby(by=['rootfolder'], dropna=False).sum()\n",
    "#print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Rolled-up to Level 1 = {df_report[root_folder].shape[0]} rows')\n",
    "\n",
    "# Roll-up to Level 2\n",
    "for idx, row in df_extract['cve_folders'].iterrows():\n",
    "    root_folder = row['name']\n",
    "    df_report[root_folder].sort_values(by=['rootfolder', 'level2', 'level3', 'level4'], inplace=True, na_position='first')\n",
    "    df_report[root_folder + '_l2'] = df_report[root_folder].groupby(by=['rootfolder', 'level2'], dropna=False).sum()\n",
    "    df_report[root_folder + '_l2'].sort_values(by=['rootfolder', 'level2'], inplace=True, na_position='first')\n",
    "    df_report[root_folder + '_l2'].to_excel(path + f'CVE - Data Analysis - {root_folder + \"_l2\"}.xlsx')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Rolled-up to Level 2 = {df_report[root_folder + \"_l2\"].shape[0]} rows')\n",
    "\n",
    "# Roll-up to Level 3\n",
    "root_folder = 'Corporate Finance'\n",
    "df_report[root_folder].sort_values(by=['rootfolder', 'level2', 'level3', 'level4'], inplace=True, na_position='first')\n",
    "df_report[root_folder + '_l3'] = df_report[root_folder].groupby(by=['rootfolder', 'level2', 'level3'], dropna=False).sum()\n",
    "df_report[root_folder + '_l3'].sort_values(by=['rootfolder', 'level2', 'level3'], inplace=True, na_position='first')\n",
    "df_report[root_folder + '_l3'].to_excel(path + f'CVE - Data Analysis - {root_folder + \"_l3\"}.xlsx')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Rolled-up to Level 3 = {df_report[root_folder + \"_l3\"].shape[0]} rows')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1838096",
   "metadata": {},
   "source": [
    "### 3.3 Create consolidated  reports for JDE and MAXIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21a10d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for everything, so values are split in columns instead of concatenated in one column in report\n",
    "for folder in df_report.keys():\n",
    "    df_report[folder].reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36c0e746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rootfolder</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>level4</th>\n",
       "      <th>Compound Document</th>\n",
       "      <th>Compound Document_ZTAB</th>\n",
       "      <th>Compound Document_CDMS</th>\n",
       "      <th>Compound Document_JDE</th>\n",
       "      <th>Compound Document_MAX</th>\n",
       "      <th>Compound Email</th>\n",
       "      <th>...</th>\n",
       "      <th>Url</th>\n",
       "      <th>Url_ZTAB</th>\n",
       "      <th>Url_CDMS</th>\n",
       "      <th>Url_JDE</th>\n",
       "      <th>Url_MAX</th>\n",
       "      <th>Physical Item</th>\n",
       "      <th>Physical Item_ZTAB</th>\n",
       "      <th>Physical Item_CDMS</th>\n",
       "      <th>Physical Item_JDE</th>\n",
       "      <th>Physical Item_MAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Canadian Plains_NOMIGRATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Canadian Plains_NOMIGRATE</td>\n",
       "      <td>Applications</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Canadian Plains_NOMIGRATE</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Cenovus Application Support and Maintenance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Canadian Plains_NOMIGRATE</td>\n",
       "      <td>Applications</td>\n",
       "      <td>Request to Abandon Approval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Upstream Business Services</td>\n",
       "      <td>Weyburn Joint Venture</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Upstream Business Services</td>\n",
       "      <td>Weyburn Joint Venture</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Upstream Business Services</td>\n",
       "      <td>Weyburn Joint Venture</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Upstream Business Services</td>\n",
       "      <td>Weyburn Joint Venture</td>\n",
       "      <td>Application Support Related Documents</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Upstream Operations</td>\n",
       "      <td>Upstream Business Services</td>\n",
       "      <td>Weyburn Joint Venture</td>\n",
       "      <td>Weyburn Unit Annual Reports</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>646 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              rootfolder                      level2                 level3  \\\n",
       "0    Upstream Operations                         NaN                    NaN   \n",
       "1    Upstream Operations   Canadian Plains_NOMIGRATE                    NaN   \n",
       "2    Upstream Operations   Canadian Plains_NOMIGRATE           Applications   \n",
       "3    Upstream Operations   Canadian Plains_NOMIGRATE           Applications   \n",
       "4    Upstream Operations   Canadian Plains_NOMIGRATE           Applications   \n",
       "..                   ...                         ...                    ...   \n",
       "641  Upstream Operations  Upstream Business Services  Weyburn Joint Venture   \n",
       "642  Upstream Operations  Upstream Business Services  Weyburn Joint Venture   \n",
       "643  Upstream Operations  Upstream Business Services  Weyburn Joint Venture   \n",
       "644  Upstream Operations  Upstream Business Services  Weyburn Joint Venture   \n",
       "645  Upstream Operations  Upstream Business Services  Weyburn Joint Venture   \n",
       "\n",
       "                                          level4  Compound Document  \\\n",
       "0                                            NaN                NaN   \n",
       "1                                            NaN                0.0   \n",
       "2                                            NaN                0.0   \n",
       "3    Cenovus Application Support and Maintenance                0.0   \n",
       "4                    Request to Abandon Approval                0.0   \n",
       "..                                           ...                ...   \n",
       "641                                         2008                0.0   \n",
       "642                                         2009                0.0   \n",
       "643                                         2010                0.0   \n",
       "644        Application Support Related Documents                0.0   \n",
       "645                  Weyburn Unit Annual Reports                0.0   \n",
       "\n",
       "     Compound Document_ZTAB  Compound Document_CDMS  Compound Document_JDE  \\\n",
       "0                       NaN                     NaN                    NaN   \n",
       "1                       0.0                     0.0                    0.0   \n",
       "2                       0.0                     0.0                    0.0   \n",
       "3                       0.0                     0.0                    0.0   \n",
       "4                       0.0                     0.0                    0.0   \n",
       "..                      ...                     ...                    ...   \n",
       "641                     0.0                     0.0                    0.0   \n",
       "642                     0.0                     0.0                    0.0   \n",
       "643                     0.0                     0.0                    0.0   \n",
       "644                     0.0                     0.0                    0.0   \n",
       "645                     0.0                     0.0                    0.0   \n",
       "\n",
       "     Compound Document_MAX  Compound Email  ...  Url  Url_ZTAB  Url_CDMS  \\\n",
       "0                      NaN             NaN  ...  NaN       NaN       NaN   \n",
       "1                      0.0             0.0  ...  0.0       0.0       0.0   \n",
       "2                      0.0             0.0  ...  0.0       0.0       0.0   \n",
       "3                      0.0             0.0  ...  0.0       0.0       0.0   \n",
       "4                      0.0             0.0  ...  0.0       0.0       0.0   \n",
       "..                     ...             ...  ...  ...       ...       ...   \n",
       "641                    0.0             0.0  ...  0.0       0.0       0.0   \n",
       "642                    0.0             0.0  ...  0.0       0.0       0.0   \n",
       "643                    0.0             0.0  ...  0.0       0.0       0.0   \n",
       "644                    0.0             0.0  ...  0.0       0.0       0.0   \n",
       "645                    0.0             0.0  ...  0.0       0.0       0.0   \n",
       "\n",
       "     Url_JDE  Url_MAX  Physical Item  Physical Item_ZTAB  Physical Item_CDMS  \\\n",
       "0        NaN      NaN            NaN                 NaN                 NaN   \n",
       "1        0.0      0.0            0.0                 0.0                 0.0   \n",
       "2        0.0      0.0            0.0                 0.0                 0.0   \n",
       "3        0.0      0.0            0.0                 0.0                 0.0   \n",
       "4        0.0      0.0            0.0                 0.0                 0.0   \n",
       "..       ...      ...            ...                 ...                 ...   \n",
       "641      0.0      0.0            0.0                 0.0                 0.0   \n",
       "642      0.0      0.0            0.0                 0.0                 0.0   \n",
       "643      0.0      0.0            0.0                 0.0                 0.0   \n",
       "644      0.0      0.0            0.0                 0.0                 0.0   \n",
       "645      0.0      0.0            0.0                 0.0                 0.0   \n",
       "\n",
       "     Physical Item_JDE  Physical Item_MAX  \n",
       "0                  NaN                NaN  \n",
       "1                  0.0                0.0  \n",
       "2                  0.0                0.0  \n",
       "3                  0.0                0.0  \n",
       "4                  0.0                0.0  \n",
       "..                 ...                ...  \n",
       "641                0.0                0.0  \n",
       "642                0.0                0.0  \n",
       "643                0.0                0.0  \n",
       "644                0.0                0.0  \n",
       "645                0.0                0.0  \n",
       "\n",
       "[646 rows x 56 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Upstream Operations after being edited manually\n",
    "df_report['Upstream Operations'] = pd.read_excel(extract_path + 'CVE - Data Analysis - Upstream Operations-compiled.xlsx')\n",
    "df_report['Upstream Operations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf7e417c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep 26 2023 06:55:19 AM: Calculating JDE report...\n",
      "Sep 26 2023 06:55:19 AM:     Application Data\n",
      "Sep 26 2023 06:55:19 AM:     Bulkloadertest_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Business Development, Canadian Gas Marketing and Power\n",
      "Sep 26 2023 06:55:19 AM:     CL Performance Test_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Canadian Plains_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Cenovus Communities\n",
      "Sep 26 2023 06:55:19 AM:     CenovusRedirect_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Content Server Event Calendar_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Conventional Oil & Natural Gas_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Corporate\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Development\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Finance\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Relations\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Services\n",
      "Sep 26 2023 06:55:19 AM:     Deep Basin\n",
      "Sep 26 2023 06:55:19 AM:     Development & Operations Services\n",
      "Sep 26 2023 06:55:19 AM:     Down Stream\n",
      "Sep 26 2023 06:55:19 AM:     ECM Content Server\n",
      "Sep 26 2023 06:55:19 AM:     ECM Midway Content Server_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     EnCana Communities (Archived)\n",
      "Sep 26 2023 06:55:19 AM:     Executive Team_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Integrated Oil\n",
      "Sep 26 2023 06:55:19 AM:     Livelink Help OLD_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Livelink Help_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Oil Sands\n",
      "Sep 26 2023 06:55:19 AM:     Operations\n",
      "Sep 26 2023 06:55:19 AM:     Project Controls\n",
      "Sep 26 2023 06:55:19 AM:     Test Well File Management_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Well File Management\n",
      "Sep 26 2023 06:55:19 AM:     Upstream Operations\n",
      "Sep 26 2023 06:55:19 AM:     Report Output = 429 rows\n",
      "Sep 26 2023 06:55:19 AM: Calculating MAX report...\n",
      "Sep 26 2023 06:55:19 AM:     Application Data\n",
      "Sep 26 2023 06:55:19 AM:     Bulkloadertest_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Business Development, Canadian Gas Marketing and Power\n",
      "Sep 26 2023 06:55:19 AM:     CL Performance Test_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Canadian Plains_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Cenovus Communities\n",
      "Sep 26 2023 06:55:19 AM:     CenovusRedirect_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Content Server Event Calendar_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Conventional Oil & Natural Gas_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Corporate\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Development\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Finance\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Relations\n",
      "Sep 26 2023 06:55:19 AM:     Corporate Services\n",
      "Sep 26 2023 06:55:19 AM:     Deep Basin\n",
      "Sep 26 2023 06:55:19 AM:     Development & Operations Services\n",
      "Sep 26 2023 06:55:19 AM:     Down Stream\n",
      "Sep 26 2023 06:55:19 AM:     ECM Content Server\n",
      "Sep 26 2023 06:55:19 AM:     ECM Midway Content Server_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     EnCana Communities (Archived)\n",
      "Sep 26 2023 06:55:19 AM:     Executive Team_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Integrated Oil\n",
      "Sep 26 2023 06:55:19 AM:     Livelink Help OLD_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Livelink Help_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Oil Sands\n",
      "Sep 26 2023 06:55:19 AM:     Operations\n",
      "Sep 26 2023 06:55:19 AM:     Project Controls\n",
      "Sep 26 2023 06:55:19 AM:     Test Well File Management_NOMIGRATE\n",
      "Sep 26 2023 06:55:19 AM:     Well File Management\n",
      "Sep 26 2023 06:55:20 AM:     Upstream Operations\n",
      "Sep 26 2023 06:55:20 AM:     Report Output = 1092 rows\n"
     ]
    }
   ],
   "source": [
    "# Consolidated Report of Folders with JDE and MAXIMO Links\n",
    "\n",
    "for root_folder in ['JDE', 'MAX']:\n",
    "    df_report[root_folder] = pd.DataFrame()\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Calculating {root_folder} report...')\n",
    "    for folder in df_report.keys():\n",
    "        if folder.find('_l3') == -1 and folder != 'JDE' and folder != 'MAX' and folder != 'Enterprise RM Workspace_NOMIGRATE' and folder != 'Planning & Exploration':\n",
    "            print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     {folder}')\n",
    "            df = df_report[folder][['rootfolder', 'level2', 'level3', 'level4', 'Document', 'Document_ZTAB', 'Document_CDMS', f'Document_{root_folder}', 'Folder', 'Folder_ZTAB', 'Folder_CDMS', f'Folder_{root_folder}']][df_report[folder][f'Document_{root_folder}'] + df_report[folder][f'Folder_{root_folder}'] > 0]\n",
    "            if df_report[root_folder].empty:\n",
    "                df_report[root_folder] = df\n",
    "            else:\n",
    "                df_report[root_folder] = pd.concat([df_report[root_folder], df])\n",
    "\n",
    "    # Report output\n",
    "    df_report[root_folder].set_index(['rootfolder', 'level2', 'level3', 'level4'], inplace=True)\n",
    "    df_report[root_folder].to_excel(path + f'CVE - Data Analysis - {root_folder}.xlsx')\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Report Output = {df_report[root_folder].shape[0]} rows')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c23f4",
   "metadata": {},
   "source": [
    "### 3.3 Upload Category Data and create Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe68f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of extract Folders: Root Folders\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading CVE root folders...')\n",
    "cnxn = ora_engine.connect()\n",
    "df_extract['cve_extract_folders'] = pd.read_sql(query['cve_root_folders'], cnxn)\n",
    "cnxn.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded cve_root_folders = {df_extract[\"cve_extract_folders\"].shape[0]} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of extract Folders: Specific Folder list\n",
    "cve_folders = {'dataid': [214361471, 67272049, 1221831, 257997],\n",
    "               'name': ['Deep Basin Open Invoice_Migrated to CDMS and redirecting', 'Financial Images_Migrated to CDMS & redirecting',\n",
    "                      'Corporate Finance_NOMIGRATE', 'JDE Financial PDFs and Reports_TO BE ARCHIVED'],\n",
    "               'path': ['Enterprise:Application Data:', 'Enterprise:Application Data:',\n",
    "                      'Enterprise:Corporate Finance:Comptrollers:', 'Enterprise:Corporate Finance:']\n",
    "              }\n",
    "df_extract['cve_extract_folders'] = pd.DataFrame(cve_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905184e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of extract Folders: Specific Folder list\n",
    "cve_folders = {'dataid': [67272049, 1221831, 257997, 1234487, 171378752],\n",
    "               'name': ['Financial Images_Migrated to CDMS & redirecting', 'Corporate Finance_NOMIGRATE',\n",
    "                        'JDE Financial PDFs and Reports_TO BE ARCHIVED', 'Financial Images',\n",
    "                        'JDE Attachments'],\n",
    "               'path': ['Enterprise:Application Data:', 'Enterprise:Corporate Finance:Comptrollers:',\n",
    "                        'Enterprise:Corporate Finance:', 'Enterprise:Upstream Operations:Finance:Central Services_NOMIGRATE:',\n",
    "                        'Enterprise:Upstream Operations:Upstream Business Services:Supply Management:']\n",
    "              }\n",
    "df_extract['cve_extract_folders'] = pd.DataFrame(cve_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7d11d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oct 13 2023 14:25:36 PM: ********** FOLDER = Financial Images_Migrated to CDMS & redirecting **********\n",
      "Oct 13 2023 14:25:36 PM: Loading CVE Folder and Document data...\n",
      "Oct 13 2023 14:46:33 PM:     Loaded categories = 13 rows\n",
      "Oct 13 2023 14:46:33 PM: ********** FOLDER = Corporate Finance_NOMIGRATE **********\n",
      "Oct 13 2023 14:46:33 PM: Loading CVE Folder and Document data...\n",
      "Oct 13 2023 15:00:38 PM:     Loaded categories = 9 rows\n",
      "Oct 13 2023 15:00:38 PM: ********** FOLDER = JDE Financial PDFs and Reports_TO BE ARCHIVED **********\n",
      "Oct 13 2023 15:00:38 PM: Loading CVE Folder and Document data...\n",
      "Oct 13 2023 15:14:53 PM:     Loaded categories = 6 rows\n",
      "Oct 13 2023 15:14:53 PM: ********** FOLDER = Financial Images **********\n",
      "Oct 13 2023 15:14:53 PM: Loading CVE Folder and Document data...\n",
      "Oct 13 2023 16:57:31 PM:     Loaded categories = 12 rows\n",
      "Oct 13 2023 16:57:31 PM: ********** FOLDER = JDE Attachments **********\n",
      "Oct 13 2023 16:57:31 PM: Loading CVE Folder and Document data...\n",
      "Oct 13 2023 17:02:04 PM:     Loaded categories = 1 rows\n",
      "Oct 13 2023 17:02:05 PM: Done\n"
     ]
    }
   ],
   "source": [
    "df_report['categories'] = pd.DataFrame()\n",
    "#df = df_extract['cve_root_folders'][(df_extract['cve_root_folders'].index == 31)]\n",
    "#df = df_extract['cve_root_folders'] ## takes 3 hrs 20 min for all root folders\n",
    "df = df_extract['cve_extract_folders']\n",
    "\n",
    "# Build Data Assessment Report\n",
    "# Extract Data\n",
    "for idx, row in df.iterrows():\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: ********** FOLDER = {row[\"name\"]} **********')\n",
    "    # Upload container and contents for root folder from Database\n",
    "    upload_cve_cats(row['dataid'])\n",
    "    df_extract['cve_categories'] = df_extract['cve_categories'].rename(columns={'valstr': 'Category', 'items': row[\"name\"]})\n",
    "\n",
    "    # Create Category Matrix\n",
    "    if df_report['categories'].empty:\n",
    "        df_report['categories'] = df_extract['cve_categories']\n",
    "    else:\n",
    "        df_report['categories'] = df_report['categories'].merge(df_extract['cve_categories'], how='outer', on='Category')\n",
    "df_report['categories'] = df_report['categories'].transpose(copy=False)\n",
    "df_report['categories'].to_excel(path + f'CVE - Data Analysis - Categories.xlsx')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336d3f0",
   "metadata": {},
   "source": [
    "# 4. Object Importer Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Root Folders from Database\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Loading CVE root folders...')\n",
    "cnxn = ora.connect(cnxn_str['cve'])\n",
    "df_extract['cve_root_folders'] = pd.read_sql(query['cve_root_folders'], cnxn)\n",
    "cnxn.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}:     Loaded cve_root_folders = {df_extract[\"cve_root_folders\"].shape[0]} rows')\n",
    "\n",
    "# Create OI Files\n",
    "for idx, row in df_extract['cve_root_folders'][df_extract['cve_root_folders']['DATAID'] == 213026382].iterrows():\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: ********** ROOT FOLDER = {row[\"NAME\"]} **********')\n",
    "    # Upload container and contents for root folder from Database\n",
    "#    upload_cve_data(row['DATAID'])\n",
    "    upload_cve_cats(row['DATAID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4135e6",
   "metadata": {},
   "source": [
    "# ----- SAMPLE CODE -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a5285",
   "metadata": {},
   "source": [
    "#### 2.1 Apply Mapping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ff999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.1.1 Uploads Mapping Tables\n",
    "print(f'{dt.now()}: Loading mapping tables ...')\n",
    "xls = pd.ExcelFile(path + mapping_file)\n",
    "mapping_tables = {}\n",
    "for sheet in xls.sheet_names:\n",
    "    mapping_tables['df_' + sheet] = pd.read_excel(path + mapping_file, sheet_name=sheet)\n",
    "\n",
    "# 2.1.2 Apply Mapping Tables to retrieve FOLDER_NAME\n",
    "if 'FOLDER_NAME' in df_transform.columns: df_transform.drop(['FOLDER_NAME'], axis=1, inplace=True)\n",
    "if 'TYPE' in df_transform.columns: df_transform.drop(['TYPE'], axis=1, inplace=True)\n",
    "if 'FOLDER_NAME_FILE' in df_transform.columns: df_transform.drop(['FOLDER_NAME_FILE'], axis=1, inplace=True)\n",
    "\n",
    "df_transform = df_transform.merge(mapping_tables['df_project_name_from_file'][['FILE_ID', 'FOLDER_NAME']], how='left', on='FILE_ID')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: FOLDER_NAME by File... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "df_transform = df_transform.merge(mapping_tables['df_project_name_from_prefix'], how='left', on='DOCUMENT_NUMBER', suffixes=('_FILE', None ))\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: FOLDER_NAME by Prefix... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d7e8e",
   "metadata": {},
   "source": [
    "#### 2.2 Apply Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed114cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Applying transformation to Docs... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.1 C_REVISION\n",
    "df_transform['C_REVISION'] = df_transform['REVISION'].apply(lambda x: x if pd.notnull(x) and str(x).strip() != '' else '00')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_REVISION... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.2 C_CREATE_DATE\n",
    "df_transform['C_CREATE_DATE'] = df_transform['CREATED_DATE'].apply(lambda x: '' if pd.isnull(x) \n",
    "                                                                   else (str(x)[0:10].replace('-', '') if str(x) < '2024-01-01' \n",
    "                                                                         else '19' + str(x)[2:10].replace('-', '')))\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_CREATE_DATE... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.3 C_DATE_OBSOLETE\n",
    "df_transform['C_DATE_OBSOLETE'] = df_transform['DATE_OBSOLETE'].apply(lambda x: '' if pd.isnull(x) \n",
    "                                                                   else (str(x)[0:10].replace('-', '') if str(x) < '2024-01-01' \n",
    "                                                                         else '19' + str(x)[2:10].replace('-', '')))\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_DATE_OBSOLETE... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.4 C_DATE_EFFECTIVE\n",
    "df_transform['C_DATE_EFFECTIVE'] = df_transform['DATE_EFFECTIVE'].apply(lambda x: '' if pd.isnull(x) \n",
    "                                                                   else (str(x)[0:10].replace('-', '') if str(x) < '2024-01-01' \n",
    "                                                                         else '19' + str(x)[2:10].replace('-', '')))\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_DATE_EFFECTIVE... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.5 C_REMARKS\n",
    "df_transform['C_REMARKS'] = df_transform['REMARKS'].str.replace('\\r\\n', ' ')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_REMARKS... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.6 C_SYNOPSIS\n",
    "df_transform['C_SYNOPSIS'] = df_transform['SYNOPSIS'].str.replace('\\r\\n', ' ')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_SYNOPSIS... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.7 BATCH\n",
    "batch_size = 5000\n",
    "df_batches = df_transform[['DOCUMENT_NUMBER']].drop_duplicates().reset_index(drop=True)\n",
    "df_batches['BATCH'] = pd.Series(df_batches.index.values).apply(lambda x: int(x/batch_size+1))\n",
    "if 'BATCH' in df_transform.columns: df_transform.drop(['BATCH'], axis=1, inplace=True)\n",
    "df_transform = df_transform.merge(df_batches, how='left', on='DOCUMENT_NUMBER')\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: BATCH... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.8 STATUS_LAST, TITLE_LAST - Apply status and title from latest revision to all revisions\n",
    "# To filter out all revisions of Deleted Documents, and avoit duplicates due to title\n",
    "df_last_props = df_transform[['DOCUMENT_NUMBER', 'DOCUMENT_ID']].drop_duplicates().groupby(by='DOCUMENT_NUMBER').max().reset_index()\n",
    "df_last_props = df_last_props.merge(df_transform[['DOCUMENT_NUMBER', 'DOCUMENT_ID', 'STATUS', 'TITLE']].drop_duplicates(),\n",
    "                                      how='left', on=['DOCUMENT_NUMBER', 'DOCUMENT_ID'])\n",
    "if 'STATUS_LAST' in df_transform.columns: df_transform.drop(['STATUS_LAST'], axis=1, inplace=True)\n",
    "if 'TITLE_LAST' in df_transform.columns: df_transform.drop(['TITLE_LAST'], axis=1, inplace=True)\n",
    "df_transform = df_transform.merge(df_last_props, how='left', on=['DOCUMENT_NUMBER'], suffixes=(None, '_LAST'))\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: STATUS_LAST, TITLE_LAST... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "\n",
    "# 2.2.9 C_FOLDER_TYPE\n",
    "df_transform['C_FOLDER_TYPE'] = df_transform[['STATUS_LAST', 'TYPE']].apply(lambda x: 'DELETED' if x['STATUS_LAST'] == 'Deleted' or x['TYPE'] == 'DELETED'\n",
    "                                                                                              else x['TYPE'],axis=1)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_FOLDER_TYPE... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.10 C_FOLDER_PATH\n",
    "df_transform['C_FOLDER_PATH'] = df_transform[['C_FOLDER_TYPE', 'FOLDER_NAME', 'FOLDER_NAME_FILE', 'DOCUMENT_NUMBER']].apply(lambda x: '' if x['C_FOLDER_TYPE'] == 'DELETED' or x['C_FOLDER_TYPE'] == 'DRAWING'\n",
    "                                                                                                                            else (root_folder + 'UNKNOWN:' + x['DOCUMENT_NUMBER'] if x['C_FOLDER_TYPE'] == 'UNKNOWN'\n",
    "                                                                                                                                  else (root_folder + x['FOLDER_NAME'] if x['C_FOLDER_TYPE'] != 'Y' and pd.notnull(x['FOLDER_NAME'])\n",
    "                                                                                                                                        else (root_folder + 'PROJECTS:' + x['DOCUMENT_NUMBER'] if pd.isnull(x['FOLDER_NAME'])\n",
    "                                                                                                                                              else (root_folder + 'PROJECTS:' + x['FOLDER_NAME'] if x['FOLDER_NAME'] != 'TAKE_FROM_FILE'\n",
    "                                                                                                                                                    else root_folder + 'PROJECTS:' + x['FOLDER_NAME_FILE']\n",
    "                                                                                                                                                   )))), axis=1)\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_FOLDER_PATH... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.11 C_DOCUMENT_ID\n",
    "df_transform['C_DOCUMENT_ID'] = df_transform['C_FOLDER_PATH'] + '_' + df_transform['FILE_NAME']\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_DOCUMENT_ID... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.12 C_VERSION_NAME\n",
    "df_transform['C_VERSION_NAME'] =  df_transform['FILE_ID'].astype(str) + '(rev' + df_transform['C_REVISION'].astype(str) + ') ' + df_transform['FILE_NAME']\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_VERSION_NAME... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.13 C_EBWEB_FILE_PATH\n",
    "df_transform['C_EBWEB_FILE_PATH'] = df_transform['REPOSITORY'] + df_transform['PATH']\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_EBWEB_FILE_PATH... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.14 C_EXTRACT_FILE_PATH\n",
    "df_transform['C_EXTRACT_FILE_PATH'] = df_transform['C_EBWEB_FILE_PATH'].apply(lambda x: x.replace('E:', '\\\\\\wsidm009pd'))\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_EXTRACT_FILE_PATH... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "# 2.2.15 C_UPLOAD_FILE_PATH\n",
    "df_transform['C_UPLOAD_FILE_PATH'] = upload_path + df_transform['C_VERSION_NAME']\n",
    "#df_transform['C_UPLOAD_FILE_PATH'] = upload_path + df_transform['FILE_NAME']\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: C_UPLOAD_FILE_PATH... {df_transform.shape[0]} rows affected')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e049d42",
   "metadata": {},
   "source": [
    "## Step 3. Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c1736",
   "metadata": {},
   "source": [
    "#### 3.0 Create Assessment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0.1 Raw Column Counts and Single Column Lookups\n",
    "def column_assessment(df:pd.DataFrame, col_lookup_skip:list):\n",
    "    df_col_counts = pd.DataFrame()\n",
    "    col_len = {}\n",
    "    col_newline = {}\n",
    "    col_lookup_vals = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        count_newline = 0\n",
    "        col_len[col] = int(df[col].astype(str).str.len().max())\n",
    "        col_newline[col] = int(df[col].astype('str').str.find('\\r\\n').apply(lambda x: 1 if x > 0 else 0).sum())\n",
    "        if col not in col_lookup_skip:\n",
    "            col_lookup_vals[col] = df[[col, 'DOCUMENT_ID']].groupby(by=col).count()\n",
    "\n",
    "    df_col_counts = pd.DataFrame(df.count()).rename(columns={0: 'Count All'})\n",
    "    df_col_counts = df_col_counts.merge(pd.DataFrame(df.nunique()).rename(columns={0: 'Count Unique'})\n",
    "                                        ,how='outer', right_index=True, left_index=True)\n",
    "    df_col_counts = df_col_counts.merge(pd.DataFrame(pd.Series(col_len), columns=['Max Length'])\n",
    "                                        ,how='outer', right_index=True, left_index=True)\n",
    "    df_col_counts = df_col_counts.merge(pd.DataFrame(pd.Series(col_newline), columns=['With New Line char'])\n",
    "                                        ,how='outer', right_index=True, left_index=True)\n",
    "    return df_col_counts, col_lookup_vals\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebc55c",
   "metadata": {},
   "source": [
    "#### 3.1 Column Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.1 Column Data Assessment\n",
    "lookup_skip_columns = ['DOCUMENT_ID', 'DOCUMENT_NUMBER', 'PROJECT_CODE', 'TITLE']\n",
    "df_col_counts, col_lookup_vals = column_assessment(df_extract , lookup_skip_columns) \n",
    "\n",
    "with pd.ExcelWriter(path + 'Lima - Project - Columns Data Assessment.xlsx') as writer:\n",
    "    df_col_counts.to_excel(writer, sheet_name='Counts')\n",
    "    for col in col_lookup_vals:\n",
    "        if not col_lookup_vals[col].empty: col_lookup_vals[col].to_excel(writer, sheet_name = 'Values ' + col)\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c825d",
   "metadata": {},
   "source": [
    "#### 3.3 Validation & Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85128b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3.3.1.a Duplicated File Ids in EXTRACT\n",
    "# (cartesian product: Caused by having same file associated to 2 different copies in eBWeb)\n",
    "df_dup_file_id_copy = df_extract[['FILE_ID', 'DOCUMENT_ID']].groupby(by='FILE_ID').count()\n",
    "df_dup_file_id_copy = df_dup_file_id_copy[(df_dup_file_id_copy['DOCUMENT_ID'] > 1)]\n",
    "print(f'Same File Ids in multiple copies: \\n {df_dup_file_id_copy}')\n",
    "\n",
    "# 3.3.1.b Duplicated File Ids in TRASNFORM\n",
    "# (NOT an error: Copied same files to multiple target folders per user request)\n",
    "df_dup_file_id_folder = df_transform[['FILE_ID', 'DOCUMENT_ID']].groupby(by='FILE_ID').count()\n",
    "df_dup_file_id_folder = df_dup_file_id_folder[(df_dup_file_id_folder['DOCUMENT_ID'] > 1)]\n",
    "print(f'\\n Same File Ids in multiple target folders: \\n {df_dup_file_id_folder}')\n",
    "\n",
    "# 3.3.2 Duplicated Document Names in same Folder\n",
    "df_dup_docs = df_transform[['C_FOLDER_PATH', 'FILE_NAME', 'TYPE']][(df_transform['C_FOLDER_TYPE'] != 'DELETED')].drop_duplicates().groupby(by=['C_FOLDER_PATH', 'FILE_NAME']).count()\n",
    "df_dup_docs = df_dup_docs[df_dup_docs['TYPE']>1]\n",
    "print(f'\\n Duplicated File Names within Target Folders: \\n {df_dup_docs}')\n",
    "\n",
    "# 3.3.3 Duplicated Filenames in each Revision\n",
    "#(catesian product: created by dup DOCUMENT_NUMBER in the mapping table. \n",
    "#Dups are caused by the same PREFIX having different TITLE in each revisions)\n",
    "df_dup_revs = df_transform[['C_FOLDER_PATH', 'C_VERSION_NAME', 'C_REVISION', 'TYPE']][(df_transform['C_FOLDER_TYPE'] != 'DELETED')].groupby(by=['C_FOLDER_PATH', 'C_VERSION_NAME', 'C_REVISION']).count()\n",
    "df_dup_revs = df_dup_revs[df_dup_revs['TYPE']>1]\n",
    "print(f'\\n Document Numbers with diff Title in multiple revisions causing filenames cartesian products: \\n {df_dup_revs}')\n",
    "\n",
    "# 3.3.4 Documents with missing mapping\n",
    "df_missing_mapping = df_transform[['PROJECT_CODE', 'TITLE', 'DOCUMENT_NUMBER', 'STATUS']][(df_transform['C_FOLDER_TYPE'].isnull())].drop_duplicates()\n",
    "print(f'\\n Document Numbers without a type/folder mapping: \\n {df_missing_mapping}')\n",
    "\n",
    "# 3.3.5 Documents with multiple versions\n",
    "df_mult_vers = df_transform[['C_FOLDER_PATH', 'FILE_NAME', 'FILE_ID']][(df_transform['C_FOLDER_TYPE'] != 'DELETED')].groupby(by=['C_FOLDER_PATH', 'FILE_NAME']).count()\n",
    "df_mult_vers = df_mult_vers[df_mult_vers['FILE_ID']>1]\n",
    "\n",
    "# 3.3.6 Counts per FOLDER TYPE\n",
    "df_count_per_folder = df_transform[['C_FOLDER_TYPE', 'C_FOLDER_PATH', 'DOCUMENT_ID', 'C_DOCUMENT_ID', 'FILE_ID', 'FILE_SIZE']].groupby(by=['C_FOLDER_TYPE'], dropna=False).agg({'C_FOLDER_PATH': pd.Series.nunique, 'DOCUMENT_ID': pd.Series.nunique, 'C_DOCUMENT_ID': pd.Series.nunique, 'FILE_ID': pd.Series.count, 'FILE_SIZE': lambda x: np.sum(x)/1024/1024/1024})\n",
    "df_count_per_folder.rename(columns={'C_FOLDER_PATH': 'Folders', 'DOCUMENT_ID': 'Folders_Revisions', 'C_DOCUMENT_ID': 'Documents', 'FILE_ID': 'Versions'}, inplace=True)\n",
    "\n",
    "# 3.3.7 Counts per FOLDER TYPE and BATCH\n",
    "df_count_per_batch = df_transform[['C_FOLDER_TYPE', 'BATCH', 'C_FOLDER_PATH', 'C_DOCUMENT_ID', 'FILE_ID', 'FILE_SIZE']].groupby(by=['C_FOLDER_TYPE', 'BATCH'], dropna=False).agg({'C_FOLDER_PATH': pd.Series.nunique, 'C_DOCUMENT_ID': pd.Series.nunique, 'FILE_ID': pd.Series.count, 'FILE_SIZE': lambda x: np.sum(x)/1024/1024/1024})\n",
    "df_count_per_batch.rename(columns={'C_FOLDER_PATH': 'Folders', 'C_DOCUMENT_ID': 'Documents', 'FILE_ID': 'Versions'}, inplace=True)\n",
    "\n",
    "with pd.ExcelWriter(path + 'Lima - Project - Counts.xlsx') as writer:\n",
    "    df_dup_file_id_copy.to_excel(writer, sheet_name='Dup Files in many copies')\n",
    "    df_dup_file_id_folder.to_excel(writer, sheet_name='Dup Files in many folders')\n",
    "    df_dup_docs.to_excel(writer, sheet_name='Dup Documents in same folder')\n",
    "    df_dup_revs.to_excel(writer, sheet_name='Dup Files in same revision')\n",
    "    df_missing_mapping.to_excel(writer, sheet_name='Missing Mapping')\n",
    "    df_mult_vers.to_excel(writer, sheet_name='Multiple Versions')\n",
    "    df_count_per_folder.to_excel(writer, sheet_name='Counts per Folder')\n",
    "    df_count_per_batch.to_excel(writer, sheet_name='Counts per Batch')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37187d",
   "metadata": {},
   "source": [
    "## Step 4. Load Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07571298",
   "metadata": {},
   "source": [
    "#### 4.1 XML DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91328bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "migration_date = dt.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# 4.1.1 Create XML Documents DataFrame\n",
    "for ft in df_transform['C_FOLDER_TYPE'].unique():\n",
    "    xml = {}\n",
    "    df = df_transform[(df_transform['C_FOLDER_TYPE'] == ft)].sort_values(by=['C_FOLDER_PATH', 'FILE_NAME'])\n",
    "    nodes = len(df)\n",
    "    xml = {'title': df['FILE_NAME'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'location': df['C_FOLDER_PATH'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'description': df['TITLE_LAST'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'created': df['C_CREATE_DATE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'createdby': ['Admin'] * nodes,\n",
    "           'modified': [migration_date] * nodes,\n",
    "           # Version\n",
    "           'filename': df['C_VERSION_NAME'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'vercdate': df['C_CREATE_DATE'].apply(lambda x: str(x) if pd.notnull(x) else 0),\n",
    "           'file': df['C_UPLOAD_FILE_PATH'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'mime': df['MIME_TYPE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           # Id's\n",
    "           'unique_doc_id': df['C_DOCUMENT_ID'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'batch': df['BATCH'],\n",
    "           'ebweb_path': df['C_EBWEB_FILE_PATH'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'extract_path': df['C_EXTRACT_FILE_PATH'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           # Categories\n",
    "           'Number': df['DOCUMENT_NUMBER'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Title': df['TITLE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Revision': df['REVISION'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Units': df['UNITS'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Document Type': df['DOCUMENT_TYPE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Document Classification': df['DOCUMENT_CLASSIFICATION'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Status': df['STATUS'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Document Status': df['DOCUMENT_STATUS'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Date Obsolete': df['C_DATE_OBSOLETE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Date Effective': df['C_DATE_EFFECTIVE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Paper Destroyed': df['PAPER_DESTROYED'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Vendor Name': df['VENDOR_NAME'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Vendor Doc Number': df['VENDOR_DOC_NUMBER'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Remarks': df['C_REMARKS'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Synopsis': df['C_SYNOPSIS'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Project Codes': df['PROJECT_CODE'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Document Id': df['DOCUMENT_ID'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'File Id': df['FILE_ID'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Repository': df['REPOSITORY'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'Path': df['PATH'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'File Name': df['FILE_NAME'].apply(lambda x: str(x).strip() if pd.notnull(x) else ''),\n",
    "           'File Size': df['FILE_SIZE'].apply(lambda x: str(x).strip() if pd.notnull(x) else '')          \n",
    "          }\n",
    "    df_xml[ft] = pd.DataFrame(xml)\n",
    "    print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Documents {ft} = {df_xml[ft].shape[0]} rows')\n",
    "   \n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343d042",
   "metadata": {},
   "source": [
    "#### 4.2 Copy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ef006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 Copy batch Files.\n",
    "#extract_path = 'E:\\\\eBWeb Test Export\\\\Projects\\\\'\n",
    "for ft in df_xml:\n",
    "    df = df_xml[ft]\n",
    "    batches = df['batch'][(df['batch'].notnull())].sort_values().unique()\n",
    "    for batch in batches:\n",
    "        df_batch = df[df['batch'] == batch]\n",
    "        bat_file_name = 'copy_projects-' + ft + '-' + str(batch).zfill(2)\n",
    "        bat_file = open(path + xml_path + bat_file_name  + '.bat', 'w',  encoding='utf-8')\n",
    "        cmd = f'ECHO start %time% >> \"{upload_path + bat_file_name}.log\"'\n",
    "        bat_file.write(cmd + '\\n')    \n",
    "        for key, val in df_batch.iterrows():\n",
    "            cmd = f'ECHO \"{val[\"filename\"]}\" >> \"{upload_path + bat_file_name}.log\"'\n",
    "            bat_file.write(cmd + '\\n')\n",
    "            cmd = f'IF EXIST \"{val[\"extract_path\"]}\" (ECHO F | XCOPY /Y /Q /F \"{val[\"extract_path\"]}\" \"{val[\"file\"]}\" >> \"{upload_path + bat_file_name}.log\") ELSE (ECHO \"{val[\"ebweb_path\"]}\" does not exist) >> \"{upload_path + bat_file_name}.log\"'\n",
    "            bat_file.write(cmd + '\\n')\n",
    "        cmd = f'ECHO end %time% >> \"{upload_path + bat_file_name}.log\"'\n",
    "        bat_file.write(cmd + '\\n')    \n",
    "        bat_file.close()\n",
    "        print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: {bat_file_name} = {df_batch.shape[0]}')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2275c5a",
   "metadata": {},
   "source": [
    "#### 3.3 Robocopy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca541578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.1 Robocopy: Documents, Emails, Shortcuts\n",
    "\"\"\"df = pd.DataFrame()\n",
    "\n",
    "bat_file = open(xml_output_path + 'bat_file.txt', 'w',  encoding='utf-8')\n",
    "            \n",
    "for pf in df_xml:\n",
    "    for type in ['document', 'email']:\n",
    "        df = df_xml[pf][(df_xml[pf]['type'] == type)]\n",
    "        batches = df['batch'][(df['batch'].notnull())].sort_values().unique()\n",
    "        for batch in batches:\n",
    "            copy_folder = pf.replace(' ', '_') + '-' + str(batch).zfill(2)\n",
    "            robocopy_cmd =  'robocopy \\\\\\cidvsrv08.huskyenergy.ca\\\\Divest_export\\\\WellFiles\\\\' + copy_folder +  '\\\\ ' + upload_path + copy_folder + '\\\\ /r:2 /MT:16 /log:' + upload_path + copy_folder + '.log' \n",
    "#            print(robocopy_cmd)\n",
    "            bat_file.write(robocopy_cmd + '\\n')\n",
    "bat_file.close()\n",
    "print(f'{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Done')\n",
    "\"\"\"\n",
    "#robocopy \"\\\\LIREB1\\eBWeb Test Export2\\Equipment\" \\\\cidvsrv08\\Divest_export\\eBWeb\\Equipment /r:2 /log:\\\\cidvsrv08\\Divest_export\\eBWeb\\Equipment.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64600c6b",
   "metadata": {},
   "source": [
    "#### 3.4 Object Importer Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d82768",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3.4.1 Create DOCUMENTS\n",
    "df = pd.DataFrame()\n",
    "for ft in ['Y', 'N - SPEC', 'N - REFERENCE', 'N - BORE', 'N - PROCEDURE', 'UNKNOWN']:\n",
    "    df = df_xml[ft]\n",
    "    if not df.empty:\n",
    "        print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {ft} = {df.shape[0]} rows')\n",
    "        build_create_oi(df = df,\n",
    "                        type = 'document',\n",
    "                        output_file = ft,\n",
    "                        sort_list = ['location', 'title'],\n",
    "                        legacy_id = 'unique_doc_id',\n",
    "                        category_name = 'Content Server Categories:OPR-REF-Lima:OPR-REF-Lima-Legacy Projects',\n",
    "                        cat_atts = ['Number', 'Revision', 'Title', 'Units', 'Document Type', 'Document Classification', 'Status', 'Document Status',\n",
    "                                    'Date Obsolete', 'Date Effective', 'Paper Destroyed', 'Vendor Name', 'Vendor Doc Number',\n",
    "                                    'Remarks', 'Synopsis', 'Project Codes', 'File Id']\n",
    "                       )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4c4d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.4.3 Delete DOCUMENTS\n",
    "for ft in  ['Y', 'N - SPEC', 'N - REFERENCE', 'N - BORE', 'N - PROCEDURE', 'UNKNOWN']:\n",
    "    df = df_xml[ft][['location', 'title', 'batch']].drop_duplicates()\n",
    "    if not df.empty:\n",
    "        print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {ft} = {df.shape[0]} rows')\n",
    "        build_delete_oi(df = df,\n",
    "                        type = 'document',\n",
    "                        output_file = ft,\n",
    "                        sort_list = ['location', 'title']\n",
    "                       )\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbcf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.3 Update DOCUMENTS to add Legacy eB Web category\n",
    "df = pd.DataFrame()\n",
    "for ft in ['Y', 'N - SPEC', 'N - REFERENCE', 'N - BORE', 'N - PROCEDURE', 'UNKNOWN']:\n",
    "    df = df_xml[ft]\n",
    "    if not df.empty:\n",
    "        print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {ft} = {df.shape[0]} rows')\n",
    "        build_update_oi(df = df,\n",
    "                        type = 'document',\n",
    "                        output_file = ft,\n",
    "                        sort_list = ['location', 'title'],\n",
    "                        category_name = 'Content Server Categories:Legacy eB Web',\n",
    "                        cat_atts = ['Document Id', 'File Id', 'Repository', 'Path', 'File Name', 'File Size']\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab53934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.4 FIX UNITS: Re-apply value after updating Category\n",
    "df = pd.DataFrame()\n",
    "for ft in ['Y', 'N - SPEC', 'N - REFERENCE', 'N - BORE', 'N - PROCEDURE']:\n",
    "    df = df_xml[ft]\n",
    "    if not df.empty:\n",
    "        print(f'\\n{dt.now().strftime(\"%b %d %Y %H:%M:%S %p\")}: Processing {ft} = {df.shape[0]} rows')\n",
    "        build_update_oi(df = df,\n",
    "                        type = 'document',\n",
    "                        output_file = f'fix-{ft}',\n",
    "                        sort_list = ['location', 'title'],\n",
    "                        category_name = 'Content Server Categories:OPR-REF-Lima:OPR-REF-Lima-Legacy Projects',\n",
    "                        cat_atts = ['Units']\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702fdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
